{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Usy5xmyucK-K",
        "outputId": "6d507199-a0c9-445a-8fa2-8ffdbbfd90d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Transformer-BasedGPTModel')"
      ],
      "metadata": {
        "id": "_IHlMnP0ckNO"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from GPTLanguageModel import GPTLanguageModel\n",
        "import os"
      ],
      "metadata": {
        "id": "xzJ8QVX0cTHg"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "block_size = 256 # maximum number of tokens the model can use as input at once when making predictions.\n",
        "max_iters = 25000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "n_head = 5\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSpCWXFKcgzB",
        "outputId": "71f08288-96b8-4496-d178-77f80d410735"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7d9c6f7fb910>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Transformer-BasedGPTModel/vefxistyaosani.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "6S_sahaMc8og"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dill\n",
        "import dill as pickle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_HG56Psc-G8",
        "outputId": "92b807b4-f6c2-495c-826b-6d5228b91461"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (0.3.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Transformer-BasedGPTModel/tokenizer', 'rb') as f:\n",
        "    tokenizer = pickle.load(f)"
      ],
      "metadata": {
        "id": "fv7PzrXddIlk"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
        "torch.save(data, '/content/drive/MyDrive/Transformer-BasedGPTModel/data.pt')\n",
        "#data = torch.load('/content/drive/MyDrive/Transformer-BasedGPTModel/data.pt', weights_only=True)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "2j6SwDmRdN1S"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "oOhfA2S7dgst"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = get_batch('train')\n",
        "print('inputs:')\n",
        "print(x.shape)\n",
        "print('targets:')\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ln9kv3qZpYVW",
        "outputId": "6ba92865-58ce-4431-def3-1d46aa1fa2b1"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([8, 256])\n",
            "targets:\n",
            "torch.Size([8, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "mxDkQjLudh0O"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTLanguageModel()\n",
        "print(model)\n",
        "m = model.to(device)\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmvZm7W-dkMg",
        "outputId": "e57a058a-b7a0-48f7-9dd3-c220cf4023e8"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPTLanguageModel(\n",
            "  (token_embedding_table): Embedding(67634, 32)\n",
            "  (position_embedding_table): Embedding(256, 32)\n",
            "  (blocks): Sequential(\n",
            "    (0): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-1): 2 x Head(\n",
            "            (key): Linear(in_features=32, out_features=16, bias=False)\n",
            "            (query): Linear(in_features=32, out_features=16, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=16, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedFoward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
            "          (3): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (1): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-1): 2 x Head(\n",
            "            (key): Linear(in_features=32, out_features=16, bias=False)\n",
            "            (query): Linear(in_features=32, out_features=16, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=16, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedFoward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
            "          (3): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (2): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-1): 2 x Head(\n",
            "            (key): Linear(in_features=32, out_features=16, bias=False)\n",
            "            (query): Linear(in_features=32, out_features=16, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=16, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedFoward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
            "          (3): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (3): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-1): 2 x Head(\n",
            "            (key): Linear(in_features=32, out_features=16, bias=False)\n",
            "            (query): Linear(in_features=32, out_features=16, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=16, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedFoward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
            "          (3): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (4): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-1): 2 x Head(\n",
            "            (key): Linear(in_features=32, out_features=16, bias=False)\n",
            "            (query): Linear(in_features=32, out_features=16, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=16, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedFoward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
            "          (3): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (5): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-1): 2 x Head(\n",
            "            (key): Linear(in_features=32, out_features=16, bias=False)\n",
            "            (query): Linear(in_features=32, out_features=16, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=16, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedFoward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
            "          (3): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (ln_f): LayerNorm()\n",
            "  (lm_head): Linear(in_features=32, out_features=67634, bias=True)\n",
            ")\n",
            "4.480114 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "zt2kR9Fpdpxj"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnnKcn6Ldc0n",
        "outputId": "4e08e14b-0291-4af7-ed22-471c9de55afa"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 11.1233, val loss 11.1245\n",
            "step 500: train loss 3.3250, val loss 3.3243\n",
            "step 1000: train loss 2.8359, val loss 2.8478\n",
            "step 1500: train loss 2.6470, val loss 2.6670\n",
            "step 2000: train loss 2.5354, val loss 2.5579\n",
            "step 2500: train loss 2.4440, val loss 2.4677\n",
            "step 3000: train loss 2.3687, val loss 2.4029\n",
            "step 3500: train loss 2.3047, val loss 2.3401\n",
            "step 4000: train loss 2.2476, val loss 2.2853\n",
            "step 4500: train loss 2.1969, val loss 2.2360\n",
            "step 5000: train loss 2.1398, val loss 2.1855\n",
            "step 5500: train loss 2.1005, val loss 2.1417\n",
            "step 6000: train loss 2.0606, val loss 2.1038\n",
            "step 6500: train loss 2.0320, val loss 2.0739\n",
            "step 7000: train loss 2.0061, val loss 2.0546\n",
            "step 7500: train loss 1.9797, val loss 2.0188\n",
            "step 8000: train loss 1.9639, val loss 2.0006\n",
            "step 8500: train loss 1.9544, val loss 1.9993\n",
            "step 9000: train loss 1.9364, val loss 1.9840\n",
            "step 9500: train loss 1.9250, val loss 1.9706\n",
            "step 10000: train loss 1.9104, val loss 1.9535\n",
            "step 10500: train loss 1.9090, val loss 1.9517\n",
            "step 11000: train loss 1.8925, val loss 1.9325\n",
            "step 11500: train loss 1.8831, val loss 1.9265\n",
            "step 12000: train loss 1.8758, val loss 1.9195\n",
            "step 12500: train loss 1.8664, val loss 1.9169\n",
            "step 13000: train loss 1.8726, val loss 1.9178\n",
            "step 13500: train loss 1.8569, val loss 1.9011\n",
            "step 14000: train loss 1.8544, val loss 1.8985\n",
            "step 14500: train loss 1.8428, val loss 1.8938\n",
            "step 15000: train loss 1.8509, val loss 1.8948\n",
            "step 15500: train loss 1.8380, val loss 1.8856\n",
            "step 16000: train loss 1.8274, val loss 1.8778\n",
            "step 16500: train loss 1.8297, val loss 1.8768\n",
            "step 17000: train loss 1.8217, val loss 1.8717\n",
            "step 17500: train loss 1.8161, val loss 1.8709\n",
            "step 18000: train loss 1.8097, val loss 1.8577\n",
            "step 18500: train loss 1.8103, val loss 1.8663\n",
            "step 19000: train loss 1.8085, val loss 1.8492\n",
            "step 19500: train loss 1.7994, val loss 1.8513\n",
            "step 20000: train loss 1.8000, val loss 1.8501\n",
            "step 20500: train loss 1.7955, val loss 1.8496\n",
            "step 21000: train loss 1.7914, val loss 1.8438\n",
            "step 21500: train loss 1.7869, val loss 1.8338\n",
            "step 22000: train loss 1.7939, val loss 1.8423\n",
            "step 22500: train loss 1.7844, val loss 1.8342\n",
            "step 23000: train loss 1.7852, val loss 1.8369\n",
            "step 23500: train loss 1.7869, val loss 1.8361\n",
            "step 24000: train loss 1.7780, val loss 1.8343\n",
            "step 24500: train loss 1.7793, val loss 1.8291\n",
            "step 24999: train loss 1.7743, val loss 1.8375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Transformer-BasedGPTModel/GPTModel', 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/Transformer-BasedGPTModel/GPTModel', 'rb') as f:\n",
        "    gpt_model = pickle.load(f)"
      ],
      "metadata": {
        "id": "0pL13qtAzpvj"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(tokenizer.decode(gpt_model.generate(context, max_new_tokens=200)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikBOWj1InyFq",
        "outputId": "efb415e6-5c6f-4d5c-e970-93e08c683f7a"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "და ამარგომა!”\n",
            "ღრეჯი ნელი შეორნიტა\n",
            "აწვიდე, შესწრალი ვერა უმუნდომართა, ვაწვრ გეხდა მონარი,\n",
            "და ბედი აღმან ფა სიდოფობესა მიგი ჭირით მიტკინდეს დადიდით;\n",
            "ზხინასა ნაახვის, კვლა მიჰკვდეს, მის ხვდეს ლხითა-მარ\n",
            "ფრომეს ცე\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "მიძერებუნ მისი მატანი დავ-და ვარდი.\n",
        "\n",
        "ჩემცად მათი ჩემნსა მბავისათლა, ჰტყვილე, მოვეტომცა საყულისა.\n",
        "მოშო, ვარ დაჯობი, კაშინისა რა სიკულებო ჟამათა.\n",
        "მეფეს სახლი, უმძმომენე, შენუ სადგების ვერთგან სგვახოსა საბარულგულობა;\n",
        "და ასა შესწრვი ვაცა ვარგთალო, ღმერთაცა მიენ,\n",
        "ჩემნ მის გულ ვაუწერათ ვითა შემოარულთა ულხერებს!\n",
        "\n",
        "\n",
        "“რაცვის ქვეცობ ამოატკაოს, მომგცე ერთითქცა აქებელულსა”.\n",
        "კაცითა ჭიანობულნი ჩვრსა გშუმოდგონეს, და ღეთი იქმართ.\n",
        "იაყამეს და და გმამისისსნა, შეარ ვკლა კვლა ესროსტყალთა.\n",
        "და მძარნებლა: “გათქვარე რაცა გითხვეთ, ჩემი ღაბად,\n",
        "და აწ აეცოდორი დავდიჭი ფირფ ხმობა კლურალვად”.\n",
        "სანურო კაცრაზებს, მოაშინესა, ლოქითად ჭამჭვნით ნაუწქება;\n",
        "და ლაშქმნით ნა მეფრი ოსიან, მოგვსგულსა, ბევრა შეცნისხდით”.\n",
        "\n",
        "რაცა ყმა ელთულსაგარ არ ჰკრთობა, მონით, ლგურთ, ბრძელი ებარი ყმა და მჭანითითა;\n",
        "ვითა მქი ომაწყევის მიხარულს, საწუშარებს,\n",
        "და აუბოვე ძლეთა უმჭობენნ განდა დასით იგი კბიარნ!”\n",
        "\n",
        "“შე, დაღავჰშუსვეს დაუნდა, დახანანმცა შენია,\n",
        "მათგან უცნობა აჰყვნეთითა ჭმ[UNK:30228]ქემათქვენანითანის.\n",
        "წათ მის”.\n",
        "და, ათუთრთა ე, პყველელთა და ჰმჯაპნდა მცრზემან პირი ტუონდესია;\n",
        "თუმათ ალისაგი ხდა გაჭავშმშანესმანე უნ მზედით;\n",
        "და უბრძანნა, მოდნად ჩემსა რაშინ ზომგად მონალთა ცხამე,\n",
        "ფიმწუკიცნა მზართ -ნანდურებელად,\n",
        "და ესწმავენ და თმოცა შენთვარ ჩემგონესფადკუბით,\n",
        "\n",
        "და მასას ბაღლსა მცნი ვნახერდლია წვილოოს, მინად მოხითა რაუბობა,\n",
        "მოიშორით მიედიფდო, გლურდეტითა ატევნებდალ, ისჯა,\n",
        "მინ მისი მჭირისობა მოეკიდარვის, მან, ჩვენ უცრემლისა საქმსაზისა,\n",
        "და ერთ უნდამსავი იკირვად საგხარე, უმოვისნეს ფრარისა:\n",
        "და ავინახავსა, რა ნა ზეტარასა, ფრიდომსა თუ შასამიხო შოგიესა!”\n",
        "\n",
        "“ნავსა ლახ საღა ქალი მოეტულისა, ვახვთა უსლებურ ნენი ბარძლისა”;\n",
        "რუცა ვარეტ ყოვლასა მტისა ტყვისა, მივთოს,\n",
        "ვცოჩნნიე ცუდად ათგაგნ მუნა მჭვრომილისა.\n",
        "და ესე მე გამამე, ბრძანანის ცედს მძმალსა ბოურფთა ანსურისა?\n",
        "და შეპოვესა მიგთან, ჩვენთა შეფერდე-გწუნა, გან ყმანარა;\n",
        "და თუ ჩემ-დძქვს შენ დაშებადვეს ასადავან,\n",
        "კვლა მხარ იტის: “იღასხეს, მაგრა ღიმი მოთვილი მთვინა,\n",
        "და მათნ აჰკვივ ვერ დიბილთა, თველთა უგვითხვედ თამათათა.\n",
        "\n",
        "“მაშთა საუგობ, ამის ამბადეს, არისა ესწორად,\n",
        "ვითა საოფთაღილა, მატითვა უთხრაფად გამალთა,\n",
        "მოვმაკობნესლა, მე მისაჭვრეტსა, მისცა, გარდ გაეშან.\n",
        "და მოდე თქვა: “ჩემი მოალატანს!\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "JLYY5E8mtLuK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}