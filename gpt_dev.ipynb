{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting google\n",
      "  Downloading google-3.0.0-py2.py3-none-any.whl (45 kB)\n",
      "     -------------------------------------- 45.3/45.3 kB 555.8 kB/s eta 0:00:00\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from google) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->google) (2.3.1)\n",
      "Installing collected packages: google\n",
      "Successfully installed google-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install google"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EY3RPRtDeSGV",
    "outputId": "01ccebb4-650e-4fb7-fe02-8f6096cbe715"
   },
   "execution_count": 34,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_24460\\907070862.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mgoogle\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolab\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdrive\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mdrive\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmount\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'/content/drive'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'google.colab'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp39-cp39-win_amd64.whl (204.1 MB)\n",
      "     -------------------------------------- 204.1/204.1 MB 5.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2022.7.1)\n",
      "Collecting typing-extensions>=4.10.0\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Collecting sympy==1.13.1\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "     ---------------------------------------- 6.2/6.2 MB 6.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Installing collected packages: typing-extensions, sympy, torch\n",
      "Successfully installed sympy-1.13.1 torch-2.6.0 typing-extensions-4.12.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /packages/c1/0d/56fb07032accbfebb4555638b6002ec5678d0942da85497e40f9405ab756/torch-2.6.0-cp39-cp39-win_amd64.whl\n",
      "  WARNING: The script isympy.exe is installed in 'C:\\Users\\NiKordzakhia\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts torchfrtrace.exe and torchrun.exe are installed in 'C:\\Users\\NiKordzakhia\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "with open('C:\\\\Users\\\\NiKordzakhia\\\\Desktop\\\\Transformer-BasedGPTModel\\\\vefxistyaosani.txt', 'r', encoding='utf-8') as f:\n",
    "  data = f.read()"
   ],
   "metadata": {
    "id": "1jegEtyFenHr"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(f'Length of dataset in characters: {len(data)}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uBWgAvQXe4mR",
    "outputId": "37cd367f-cdff-4514-bb7e-ddc057fefc0b"
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in characters: 329322\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZdyCWgIHe5Vv",
    "outputId": "1980bd95-be92-40ae-ae28-9d01caf5109a"
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\",-.:;?«»აბგდევზთიკლმნოპჟრსტუფქღყშჩცძწჭხჯჰ–—“”\n",
      "49\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: \"\".join([itos[c] for c in l])"
   ],
   "metadata": {
    "id": "SL8wW4A-fu6w"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "encoded_data = encode(data)\n",
    "data = torch.tensor(encoded_data, dtype = torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9bL_4iZshe4n",
    "outputId": "c2f34a65-fac0-4ea2-c52c-5eb86ac207fb"
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([329322]) torch.int64\n",
      "tensor([17, 16, 32, 42, 20, 29, 30, 35, 12, 25, 29, 12, 24, 20,  0, 15, 12, 29,\n",
      "        12, 40, 35, 20, 29, 20,  0,  0, 28, 25, 23, 16, 22, 23, 12, 24,  1, 36,\n",
      "        16, 33, 23, 24, 12,  1, 29, 12, 23, 35, 12, 28, 25,  1, 39, 12, 22, 20,\n",
      "        19, 12,  1, 23, 20, 19,  1, 39, 22, 20, 16, 28, 20, 19, 12,  4,  0, 18,\n",
      "        16, 14, 12, 28, 15, 23, 25,  1, 12, 28, 29, 24, 20,  1, 29, 31, 22, 20,\n",
      "        19, 12,  1, 35, 17, 24, 12,  1, 18, 16])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "n = int(len(data) * 0.9)\n",
    "train_data = data[:n]\n",
    "valid_data = data[n:]"
   ],
   "metadata": {
    "id": "R99IbOTyjP8M"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "block_size = 8\n",
    "train_data[1:block_size + 1]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12IaMz5MlKp6",
    "outputId": "48550e50-6dbd-49f8-94b0-91af96c5a121"
   },
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([16, 32, 42, 20, 29, 30, 35, 12])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for i in range(block_size):\n",
    "  context = x[:i+1]\n",
    "  target = y[i]\n",
    "  print(f\"Context: {context}; Target: {target}.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FRqYoalblSSF",
    "outputId": "7c34a521-2cdb-4205-acd0-429caaf2ab69"
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: tensor([17]); Target: 16.\n",
      "Context: tensor([17, 16]); Target: 32.\n",
      "Context: tensor([17, 16, 32]); Target: 42.\n",
      "Context: tensor([17, 16, 32, 42]); Target: 20.\n",
      "Context: tensor([17, 16, 32, 42, 20]); Target: 29.\n",
      "Context: tensor([17, 16, 32, 42, 20, 29]); Target: 30.\n",
      "Context: tensor([17, 16, 32, 42, 20, 29, 30]); Target: 35.\n",
      "Context: tensor([17, 16, 32, 42, 20, 29, 30, 35]); Target: 12.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "torch.manual_seed(1)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "  data = train_data if split == 'train' else valid_data\n",
    "  ix = torch.randint(len(data) - block_size, (batch_size, )) # torch.randint(low, high, size)\n",
    "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "  return x, y\n",
    "\n",
    "xb, yb = get_batch('train')"
   ],
   "metadata": {
    "id": "XqZ0qB1Al8Wn"
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "xb, yb"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ycCcCdBFnyYf",
    "outputId": "4b9d638f-39d6-4aee-f190-23b61c175e99"
   },
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[12,  1, 12, 42, 15, 12,  1, 36],\n         [40, 35, 20, 24, 16,  4,  0, 28],\n         [20, 24, 12, 42, 16,  4,  1,  5],\n         [ 1, 31, 18, 25, 23, 25,  4,  1]]),\n tensor([[ 1, 12, 42, 15, 12,  1, 36, 12],\n         [35, 20, 24, 16,  4,  0, 28, 12],\n         [24, 12, 42, 16,  4,  1,  5,  1],\n         [31, 18, 25, 23, 25,  4,  1, 29]]))"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "for i in range(batch_size):\n",
    "  for j in range(block_size):\n",
    "    context = xb[i, :j+1]\n",
    "    target = yb[i,j]\n",
    "    print(f'When the input is {context.tolist()}; target is {target}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Awt6JWQVnu9b",
    "outputId": "56d0a016-c956-4c60-d656-977dd331478f"
   },
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When the input is [12]; target is 1\n",
      "When the input is [12, 1]; target is 12\n",
      "When the input is [12, 1, 12]; target is 42\n",
      "When the input is [12, 1, 12, 42]; target is 15\n",
      "When the input is [12, 1, 12, 42, 15]; target is 12\n",
      "When the input is [12, 1, 12, 42, 15, 12]; target is 1\n",
      "When the input is [12, 1, 12, 42, 15, 12, 1]; target is 36\n",
      "When the input is [12, 1, 12, 42, 15, 12, 1, 36]; target is 12\n",
      "When the input is [40]; target is 35\n",
      "When the input is [40, 35]; target is 20\n",
      "When the input is [40, 35, 20]; target is 24\n",
      "When the input is [40, 35, 20, 24]; target is 16\n",
      "When the input is [40, 35, 20, 24, 16]; target is 4\n",
      "When the input is [40, 35, 20, 24, 16, 4]; target is 0\n",
      "When the input is [40, 35, 20, 24, 16, 4, 0]; target is 28\n",
      "When the input is [40, 35, 20, 24, 16, 4, 0, 28]; target is 12\n",
      "When the input is [20]; target is 24\n",
      "When the input is [20, 24]; target is 12\n",
      "When the input is [20, 24, 12]; target is 42\n",
      "When the input is [20, 24, 12, 42]; target is 16\n",
      "When the input is [20, 24, 12, 42, 16]; target is 4\n",
      "When the input is [20, 24, 12, 42, 16, 4]; target is 1\n",
      "When the input is [20, 24, 12, 42, 16, 4, 1]; target is 5\n",
      "When the input is [20, 24, 12, 42, 16, 4, 1, 5]; target is 1\n",
      "When the input is [1]; target is 31\n",
      "When the input is [1, 31]; target is 18\n",
      "When the input is [1, 31, 18]; target is 25\n",
      "When the input is [1, 31, 18, 25]; target is 23\n",
      "When the input is [1, 31, 18, 25, 23]; target is 25\n",
      "When the input is [1, 31, 18, 25, 23, 25]; target is 4\n",
      "When the input is [1, 31, 18, 25, 23, 25, 4]; target is 1\n",
      "When the input is [1, 31, 18, 25, 23, 25, 4, 1]; target is 29\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "xb[:2,:]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GWxFhJkXvXpx",
    "outputId": "2835be5b-a950-4206-ca3d-e30ffe0b7125"
   },
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[12,  1, 12, 42, 15, 12,  1, 36],\n        [40, 35, 20, 24, 16,  4,  0, 28]])"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "token_embedding_table = nn.Embedding(vocab_size, 2)\n",
    "token_embedding_table(xb[:2,:])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jQOk24LJvY-J",
    "outputId": "7074f393-f1cb-4532-db87-8f8a120b792e"
   },
   "execution_count": 17,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_24460\\3523318488.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mtoken_embedding_table\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mEmbedding\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvocab_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mtoken_embedding_table\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mxb\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'nn' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1)\n",
    "\n",
    "class BiggramLanguageModel(nn.Module):\n",
    "  def __init__(self, vocab_size):\n",
    "    super().__init__()\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "  def forward(self, idx, targets = None):\n",
    "    logits = self.token_embedding_table(idx) # (B, T, C) ==> Batch, Time, Channels; C ==> Number of classes (vocabulary size for classification).\n",
    "    if targets == None:\n",
    "      loss = None\n",
    "    else:\n",
    "      B, T, C = logits.shape\n",
    "      #print(\"Logits shape: \", logits.shape)\n",
    "      #print(\"Targets shape: \", targets.shape)\n",
    "      #print('-----------------------------')\n",
    "      logits = logits.view(B*T, C)\n",
    "      #print(\"Logits shape after: \", logits.shape)\n",
    "      targets = targets.view(B*T)\n",
    "      #print(\"Targets shape after: \", targets.shape)\n",
    "      loss = F.cross_entropy(logits, targets) # Expects (B, C, T) ==> CrossEntropy Loss computes loss per token, then averages over all 32 tokens.\n",
    "    return logits, loss\n",
    "  def generate(self, idx, max_new_tokens):\n",
    "    for _ in range(max_new_tokens):\n",
    "      logits, loss = self(idx)\n",
    "      print(\"Logits before: \", logits.shape)\n",
    "      logits = logits[:, -1,:] # Becomes (B, C) we need last token logits because when sentence comes we do not care about history but what was last word instead.\n",
    "      #The model generates text one token at a time, and each token generated is influenced by the previous tokens. But for each step, the context for generating the next token is only the most recent token—not the entire sequence.\n",
    "      print(\"Logits after: \", logits.shape)\n",
    "      probs = F.softmax(logits, -1)\n",
    "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) ; If probs = [0.1, 0.4, 0.3, 0.2], then calling torch.multinomial(probs, num_samples=1) will randomly select one index from this list (e.g., index 1, index 2, etc.), with the likelihood of each index being selected proportional to its probability in the list.\n",
    "      idx = torch.cat((idx, idx_next), dim = 1)\n",
    "    return idx"
   ],
   "metadata": {
    "id": "g0_0qME0qYSK"
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "m = BiggramLanguageModel(vocab_size)\n",
    "logits, loss = m.forward(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wtPKmXiDs-4u",
    "outputId": "1f15f4bd-a249-4ded-93df-af356efffe73"
   },
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 49])\n",
      "tensor(4.2583, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens = 2)[0].tolist()))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q6lFbqVO3Skg",
    "outputId": "92ee115b-958d-4cba-c479-941069a1a10d"
   },
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits before:  torch.Size([1, 1, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 2, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "\n",
      ";\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)"
   ],
   "metadata": {
    "id": "NTXz1WNp3ksw"
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "batch_size = 32\n",
    "for step in range(1000):\n",
    "  xb, yb = get_batch('train')\n",
    "  logits, loss = m(xb, yb)\n",
    "  optimizer.zero_grad(set_to_none=True)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  print(loss.item())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jsdDeFEa40s8",
    "outputId": "9e472444-e6f0-4a94-8bbe-a09ab3e0f13e"
   },
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.710899829864502\n",
      "2.5852248668670654\n",
      "2.6015992164611816\n",
      "2.6368637084960938\n",
      "2.6903347969055176\n",
      "2.7045798301696777\n",
      "2.680438756942749\n",
      "2.5757222175598145\n",
      "2.657632827758789\n",
      "2.693932294845581\n",
      "2.628763437271118\n",
      "2.5537397861480713\n",
      "2.6995577812194824\n",
      "2.673140525817871\n",
      "2.6172282695770264\n",
      "2.502389669418335\n",
      "2.638719320297241\n",
      "2.6107304096221924\n",
      "2.583033800125122\n",
      "2.616863965988159\n",
      "2.5899806022644043\n",
      "2.645350217819214\n",
      "2.641684055328369\n",
      "2.5644443035125732\n",
      "2.570164918899536\n",
      "2.635409116744995\n",
      "2.5563323497772217\n",
      "2.546370506286621\n",
      "2.5671591758728027\n",
      "2.5782039165496826\n",
      "2.6661882400512695\n",
      "2.651338815689087\n",
      "2.6852824687957764\n",
      "2.6388356685638428\n",
      "2.6901488304138184\n",
      "2.598951578140259\n",
      "2.6131062507629395\n",
      "2.692652702331543\n",
      "2.6567282676696777\n",
      "2.6476874351501465\n",
      "2.6257824897766113\n",
      "2.7189481258392334\n",
      "2.6811041831970215\n",
      "2.5137956142425537\n",
      "2.5868520736694336\n",
      "2.5966243743896484\n",
      "2.6583597660064697\n",
      "2.616643190383911\n",
      "2.6178157329559326\n",
      "2.550940990447998\n",
      "2.6226892471313477\n",
      "2.6869449615478516\n",
      "2.5333750247955322\n",
      "2.63262677192688\n",
      "2.6058056354522705\n",
      "2.7049057483673096\n",
      "2.5825741291046143\n",
      "2.6569790840148926\n",
      "2.506765604019165\n",
      "2.6176352500915527\n",
      "2.5949347019195557\n",
      "2.634948968887329\n",
      "2.63101863861084\n",
      "2.536719799041748\n",
      "2.61020565032959\n",
      "2.527761936187744\n",
      "2.567448377609253\n",
      "2.6362643241882324\n",
      "2.6573002338409424\n",
      "2.60758376121521\n",
      "2.6135294437408447\n",
      "2.508343458175659\n",
      "2.61824631690979\n",
      "2.6111702919006348\n",
      "2.629467725753784\n",
      "2.657249689102173\n",
      "2.5957419872283936\n",
      "2.578155755996704\n",
      "2.5800700187683105\n",
      "2.6251657009124756\n",
      "2.572232484817505\n",
      "2.5724897384643555\n",
      "2.713024377822876\n",
      "2.5247139930725098\n",
      "2.6019773483276367\n",
      "2.6757521629333496\n",
      "2.6954596042633057\n",
      "2.5544326305389404\n",
      "2.6059703826904297\n",
      "2.642970085144043\n",
      "2.538973093032837\n",
      "2.6038248538970947\n",
      "2.6393675804138184\n",
      "2.6813886165618896\n",
      "2.5761890411376953\n",
      "2.6408236026763916\n",
      "2.6583163738250732\n",
      "2.6022989749908447\n",
      "2.5516977310180664\n",
      "2.6936938762664795\n",
      "2.6996219158172607\n",
      "2.6613752841949463\n",
      "2.526073694229126\n",
      "2.650879383087158\n",
      "2.5945627689361572\n",
      "2.5585474967956543\n",
      "2.59932279586792\n",
      "2.564283609390259\n",
      "2.595411777496338\n",
      "2.6229076385498047\n",
      "2.646881580352783\n",
      "2.647177219390869\n",
      "2.5602235794067383\n",
      "2.517336845397949\n",
      "2.633089065551758\n",
      "2.6255102157592773\n",
      "2.539639472961426\n",
      "2.6649816036224365\n",
      "2.5363237857818604\n",
      "2.5969128608703613\n",
      "2.7778148651123047\n",
      "2.5951411724090576\n",
      "2.6078217029571533\n",
      "2.5568642616271973\n",
      "2.7204949855804443\n",
      "2.5810153484344482\n",
      "2.6323888301849365\n",
      "2.6096410751342773\n",
      "2.7100188732147217\n",
      "2.6141350269317627\n",
      "2.5568089485168457\n",
      "2.6605894565582275\n",
      "2.570685625076294\n",
      "2.6773669719696045\n",
      "2.556661605834961\n",
      "2.6341211795806885\n",
      "2.512446641921997\n",
      "2.5669054985046387\n",
      "2.5525219440460205\n",
      "2.5851986408233643\n",
      "2.4938323497772217\n",
      "2.554600477218628\n",
      "2.6523122787475586\n",
      "2.581317901611328\n",
      "2.699127674102783\n",
      "2.664816379547119\n",
      "2.599701166152954\n",
      "2.5295286178588867\n",
      "2.6014082431793213\n",
      "2.790942430496216\n",
      "2.6252052783966064\n",
      "2.5292840003967285\n",
      "2.5796127319335938\n",
      "2.562009572982788\n",
      "2.5579371452331543\n",
      "2.5527946949005127\n",
      "2.5669219493865967\n",
      "2.6074984073638916\n",
      "2.5294549465179443\n",
      "2.592660427093506\n",
      "2.6304948329925537\n",
      "2.6909265518188477\n",
      "2.5723166465759277\n",
      "2.571831464767456\n",
      "2.6098179817199707\n",
      "2.579123020172119\n",
      "2.5951852798461914\n",
      "2.670795202255249\n",
      "2.630444288253784\n",
      "2.516866445541382\n",
      "2.6047677993774414\n",
      "2.5717661380767822\n",
      "2.5793776512145996\n",
      "2.6698782444000244\n",
      "2.5222599506378174\n",
      "2.5524110794067383\n",
      "2.6216022968292236\n",
      "2.5375189781188965\n",
      "2.608576774597168\n",
      "2.491274118423462\n",
      "2.584639072418213\n",
      "2.5617475509643555\n",
      "2.6023881435394287\n",
      "2.6352553367614746\n",
      "2.655613422393799\n",
      "2.6651906967163086\n",
      "2.59981107711792\n",
      "2.601361036300659\n",
      "2.597445487976074\n",
      "2.5526535511016846\n",
      "2.6803433895111084\n",
      "2.5094428062438965\n",
      "2.6183359622955322\n",
      "2.521070718765259\n",
      "2.530773878097534\n",
      "2.544337511062622\n",
      "2.592449903488159\n",
      "2.7218072414398193\n",
      "2.63100004196167\n",
      "2.650449514389038\n",
      "2.5633387565612793\n",
      "2.5656895637512207\n",
      "2.6406137943267822\n",
      "2.6080400943756104\n",
      "2.5819203853607178\n",
      "2.6290173530578613\n",
      "2.50142502784729\n",
      "2.612971782684326\n",
      "2.5640852451324463\n",
      "2.505121946334839\n",
      "2.5726993083953857\n",
      "2.5677366256713867\n",
      "2.5516176223754883\n",
      "2.7379395961761475\n",
      "2.4612607955932617\n",
      "2.5987296104431152\n",
      "2.5515546798706055\n",
      "2.573965549468994\n",
      "2.571732521057129\n",
      "2.652524948120117\n",
      "2.690548896789551\n",
      "2.6059744358062744\n",
      "2.6379730701446533\n",
      "2.5163300037384033\n",
      "2.632941961288452\n",
      "2.6212730407714844\n",
      "2.547431230545044\n",
      "2.4620609283447266\n",
      "2.5506417751312256\n",
      "2.6590821743011475\n",
      "2.594697952270508\n",
      "2.572559356689453\n",
      "2.606419801712036\n",
      "2.524028778076172\n",
      "2.557490587234497\n",
      "2.6009042263031006\n",
      "2.6419997215270996\n",
      "2.6183364391326904\n",
      "2.67257022857666\n",
      "2.4161837100982666\n",
      "2.5336644649505615\n",
      "2.5722854137420654\n",
      "2.671419620513916\n",
      "2.492709159851074\n",
      "2.5432775020599365\n",
      "2.6246674060821533\n",
      "2.5242693424224854\n",
      "2.6989288330078125\n",
      "2.6200337409973145\n",
      "2.583272695541382\n",
      "2.57889986038208\n",
      "2.539260149002075\n",
      "2.624544858932495\n",
      "2.5735340118408203\n",
      "2.61677885055542\n",
      "2.58414888381958\n",
      "2.689122200012207\n",
      "2.5154964923858643\n",
      "2.565007209777832\n",
      "2.6842288970947266\n",
      "2.5931990146636963\n",
      "2.562103748321533\n",
      "2.663564443588257\n",
      "2.6760237216949463\n",
      "2.487375497817993\n",
      "2.604950428009033\n",
      "2.590247869491577\n",
      "2.5574870109558105\n",
      "2.616471767425537\n",
      "2.6593704223632812\n",
      "2.516319751739502\n",
      "2.5655438899993896\n",
      "2.5111374855041504\n",
      "2.6195476055145264\n",
      "2.6007039546966553\n",
      "2.627906322479248\n",
      "2.700829267501831\n",
      "2.662912607192993\n",
      "2.6011900901794434\n",
      "2.5663909912109375\n",
      "2.5858800411224365\n",
      "2.6208767890930176\n",
      "2.6344029903411865\n",
      "2.74837589263916\n",
      "2.6756575107574463\n",
      "2.5225939750671387\n",
      "2.56750750541687\n",
      "2.455130100250244\n",
      "2.5433292388916016\n",
      "2.548570394515991\n",
      "2.612346887588501\n",
      "2.5985634326934814\n",
      "2.5984296798706055\n",
      "2.6855733394622803\n",
      "2.630042552947998\n",
      "2.610889434814453\n",
      "2.669875144958496\n",
      "2.5887629985809326\n",
      "2.4232521057128906\n",
      "2.6283881664276123\n",
      "2.4723527431488037\n",
      "2.66219162940979\n",
      "2.5661239624023438\n",
      "2.6644041538238525\n",
      "2.603813648223877\n",
      "2.578991174697876\n",
      "2.514498472213745\n",
      "2.5924084186553955\n",
      "2.5547447204589844\n",
      "2.518761157989502\n",
      "2.601516008377075\n",
      "2.627542734146118\n",
      "2.614415168762207\n",
      "2.5853965282440186\n",
      "2.5080933570861816\n",
      "2.5448453426361084\n",
      "2.613675117492676\n",
      "2.4935266971588135\n",
      "2.665449857711792\n",
      "2.6617043018341064\n",
      "2.657560110092163\n",
      "2.6186490058898926\n",
      "2.542877197265625\n",
      "2.5750107765197754\n",
      "2.6284472942352295\n",
      "2.574557304382324\n",
      "2.488670587539673\n",
      "2.56406831741333\n",
      "2.5402073860168457\n",
      "2.5169639587402344\n",
      "2.571265935897827\n",
      "2.5803887844085693\n",
      "2.6301286220550537\n",
      "2.533186674118042\n",
      "2.7125725746154785\n",
      "2.6343679428100586\n",
      "2.618741512298584\n",
      "2.497380256652832\n",
      "2.519822359085083\n",
      "2.625544548034668\n",
      "2.584120273590088\n",
      "2.651670455932617\n",
      "2.7085518836975098\n",
      "2.6025025844573975\n",
      "2.6254825592041016\n",
      "2.635084867477417\n",
      "2.532719850540161\n",
      "2.514943838119507\n",
      "2.4782233238220215\n",
      "2.5755972862243652\n",
      "2.5532760620117188\n",
      "2.5760765075683594\n",
      "2.524543523788452\n",
      "2.6115942001342773\n",
      "2.5346994400024414\n",
      "2.5695912837982178\n",
      "2.5867185592651367\n",
      "2.6311607360839844\n",
      "2.674304962158203\n",
      "2.665350914001465\n",
      "2.573509454727173\n",
      "2.5880563259124756\n",
      "2.6411454677581787\n",
      "2.5530707836151123\n",
      "2.6760056018829346\n",
      "2.481687545776367\n",
      "2.552353858947754\n",
      "2.6599247455596924\n",
      "2.588789701461792\n",
      "2.5265591144561768\n",
      "2.4776558876037598\n",
      "2.617360830307007\n",
      "2.550241708755493\n",
      "2.397576093673706\n",
      "2.6161844730377197\n",
      "2.544638156890869\n",
      "2.597688674926758\n",
      "2.4482851028442383\n",
      "2.6398582458496094\n",
      "2.5701723098754883\n",
      "2.629923105239868\n",
      "2.478849411010742\n",
      "2.567967414855957\n",
      "2.5264546871185303\n",
      "2.6162848472595215\n",
      "2.620086431503296\n",
      "2.635503053665161\n",
      "2.5076656341552734\n",
      "2.6671383380889893\n",
      "2.590740919113159\n",
      "2.5044209957122803\n",
      "2.630354881286621\n",
      "2.5879812240600586\n",
      "2.578522205352783\n",
      "2.599362373352051\n",
      "2.5580201148986816\n",
      "2.608269214630127\n",
      "2.596285820007324\n",
      "2.506187915802002\n",
      "2.438253879547119\n",
      "2.7232656478881836\n",
      "2.6068859100341797\n",
      "2.5217509269714355\n",
      "2.5976428985595703\n",
      "2.5694262981414795\n",
      "2.653249740600586\n",
      "2.5089640617370605\n",
      "2.5016510486602783\n",
      "2.6107373237609863\n",
      "2.5931811332702637\n",
      "2.5645198822021484\n",
      "2.5750300884246826\n",
      "2.571500301361084\n",
      "2.6093826293945312\n",
      "2.5317916870117188\n",
      "2.5484156608581543\n",
      "2.6166646480560303\n",
      "2.7014386653900146\n",
      "2.587646484375\n",
      "2.557194709777832\n",
      "2.534083604812622\n",
      "2.608055591583252\n",
      "2.5455732345581055\n",
      "2.6352241039276123\n",
      "2.652155637741089\n",
      "2.4654998779296875\n",
      "2.6287002563476562\n",
      "2.6237118244171143\n",
      "2.5915448665618896\n",
      "2.6383156776428223\n",
      "2.563476085662842\n",
      "2.561366558074951\n",
      "2.5310847759246826\n",
      "2.586117744445801\n",
      "2.62634539604187\n",
      "2.6190996170043945\n",
      "2.605483293533325\n",
      "2.5668704509735107\n",
      "2.524557113647461\n",
      "2.4919204711914062\n",
      "2.5754261016845703\n",
      "2.544459819793701\n",
      "2.4553163051605225\n",
      "2.652523994445801\n",
      "2.6261963844299316\n",
      "2.603425979614258\n",
      "2.5229947566986084\n",
      "2.595914363861084\n",
      "2.6998472213745117\n",
      "2.6004891395568848\n",
      "2.5183472633361816\n",
      "2.5087437629699707\n",
      "2.5700840950012207\n",
      "2.5396809577941895\n",
      "2.611438512802124\n",
      "2.598306655883789\n",
      "2.5983169078826904\n",
      "2.5330185890197754\n",
      "2.6350152492523193\n",
      "2.5869686603546143\n",
      "2.5755653381347656\n",
      "2.5919458866119385\n",
      "2.637739896774292\n",
      "2.5519795417785645\n",
      "2.5503411293029785\n",
      "2.533846139907837\n",
      "2.535806179046631\n",
      "2.580286741256714\n",
      "2.4943220615386963\n",
      "2.5786798000335693\n",
      "2.669705390930176\n",
      "2.531240701675415\n",
      "2.5328524112701416\n",
      "2.467522144317627\n",
      "2.5415689945220947\n",
      "2.585118055343628\n",
      "2.55452823638916\n",
      "2.5851690769195557\n",
      "2.6975297927856445\n",
      "2.6468257904052734\n",
      "2.6396570205688477\n",
      "2.592942476272583\n",
      "2.556018352508545\n",
      "2.551333427429199\n",
      "2.6116604804992676\n",
      "2.612149715423584\n",
      "2.5557687282562256\n",
      "2.7007179260253906\n",
      "2.5241928100585938\n",
      "2.627577781677246\n",
      "2.556579113006592\n",
      "2.545220375061035\n",
      "2.524901866912842\n",
      "2.5015344619750977\n",
      "2.57515287399292\n",
      "2.5504753589630127\n",
      "2.5584142208099365\n",
      "2.641491413116455\n",
      "2.520707130432129\n",
      "2.645294189453125\n",
      "2.5222456455230713\n",
      "2.592562198638916\n",
      "2.5048606395721436\n",
      "2.7116758823394775\n",
      "2.555089235305786\n",
      "2.56461501121521\n",
      "2.566274642944336\n",
      "2.608534097671509\n",
      "2.587186336517334\n",
      "2.5210049152374268\n",
      "2.6065778732299805\n",
      "2.5887513160705566\n",
      "2.5124337673187256\n",
      "2.5813710689544678\n",
      "2.468039035797119\n",
      "2.520995616912842\n",
      "2.620769500732422\n",
      "2.6139590740203857\n",
      "2.5861153602600098\n",
      "2.617985248565674\n",
      "2.5149190425872803\n",
      "2.6161890029907227\n",
      "2.597614049911499\n",
      "2.5625545978546143\n",
      "2.455531358718872\n",
      "2.5198049545288086\n",
      "2.4744250774383545\n",
      "2.595524311065674\n",
      "2.5917601585388184\n",
      "2.580002546310425\n",
      "2.637723922729492\n",
      "2.531210422515869\n",
      "2.514058828353882\n",
      "2.517991065979004\n",
      "2.6308059692382812\n",
      "2.617642402648926\n",
      "2.6822259426116943\n",
      "2.7006120681762695\n",
      "2.618927001953125\n",
      "2.5977282524108887\n",
      "2.6549813747406006\n",
      "2.5598976612091064\n",
      "2.642209053039551\n",
      "2.6443557739257812\n",
      "2.469388008117676\n",
      "2.568889617919922\n",
      "2.591872215270996\n",
      "2.653341293334961\n",
      "2.5704855918884277\n",
      "2.612398386001587\n",
      "2.570519208908081\n",
      "2.592276096343994\n",
      "2.5815765857696533\n",
      "2.5918173789978027\n",
      "2.571702003479004\n",
      "2.514772653579712\n",
      "2.58972430229187\n",
      "2.598940849304199\n",
      "2.5838286876678467\n",
      "2.50364089012146\n",
      "2.6027746200561523\n",
      "2.581132650375366\n",
      "2.4432380199432373\n",
      "2.635754346847534\n",
      "2.5984015464782715\n",
      "2.461777687072754\n",
      "2.5125975608825684\n",
      "2.6271543502807617\n",
      "2.6203291416168213\n",
      "2.5240566730499268\n",
      "2.538681745529175\n",
      "2.6813273429870605\n",
      "2.5112831592559814\n",
      "2.488813638687134\n",
      "2.629856586456299\n",
      "2.718791961669922\n",
      "2.561037302017212\n",
      "2.48191237449646\n",
      "2.473771572113037\n",
      "2.546898126602173\n",
      "2.509389877319336\n",
      "2.5953359603881836\n",
      "2.620262861251831\n",
      "2.6125593185424805\n",
      "2.5019452571868896\n",
      "2.568971633911133\n",
      "2.525853395462036\n",
      "2.5704519748687744\n",
      "2.4604365825653076\n",
      "2.58309268951416\n",
      "2.6055541038513184\n",
      "2.556119441986084\n",
      "2.4810781478881836\n",
      "2.5932512283325195\n",
      "2.702404022216797\n",
      "2.563292980194092\n",
      "2.6278934478759766\n",
      "2.59395170211792\n",
      "2.477525234222412\n",
      "2.5787014961242676\n",
      "2.6089673042297363\n",
      "2.6117148399353027\n",
      "2.5617613792419434\n",
      "2.586765766143799\n",
      "2.5022034645080566\n",
      "2.5399012565612793\n",
      "2.473020553588867\n",
      "2.531651020050049\n",
      "2.6196296215057373\n",
      "2.571094274520874\n",
      "2.5669517517089844\n",
      "2.6253113746643066\n",
      "2.5139365196228027\n",
      "2.6417455673217773\n",
      "2.5319840908050537\n",
      "2.670584201812744\n",
      "2.55965518951416\n",
      "2.4729108810424805\n",
      "2.6036810874938965\n",
      "2.6457650661468506\n",
      "2.602735996246338\n",
      "2.51419734954834\n",
      "2.5562846660614014\n",
      "2.5290234088897705\n",
      "2.5079948902130127\n",
      "2.5924758911132812\n",
      "2.51768159866333\n",
      "2.524721622467041\n",
      "2.6114165782928467\n",
      "2.573589563369751\n",
      "2.565941095352173\n",
      "2.4943079948425293\n",
      "2.5877790451049805\n",
      "2.563420534133911\n",
      "2.5643088817596436\n",
      "2.597339630126953\n",
      "2.5251171588897705\n",
      "2.512460947036743\n",
      "2.608452320098877\n",
      "2.501227378845215\n",
      "2.562410831451416\n",
      "2.6354854106903076\n",
      "2.6318564414978027\n",
      "2.4964523315429688\n",
      "2.571381092071533\n",
      "2.4664127826690674\n",
      "2.5498225688934326\n",
      "2.5454602241516113\n",
      "2.5960047245025635\n",
      "2.5776631832122803\n",
      "2.619623899459839\n",
      "2.5644538402557373\n",
      "2.507991075515747\n",
      "2.579866409301758\n",
      "2.5117385387420654\n",
      "2.660183906555176\n",
      "2.6011440753936768\n",
      "2.5684401988983154\n",
      "2.5705883502960205\n",
      "2.608750104904175\n",
      "2.579589366912842\n",
      "2.6456878185272217\n",
      "2.5077877044677734\n",
      "2.532318592071533\n",
      "2.6786038875579834\n",
      "2.476933002471924\n",
      "2.586184024810791\n",
      "2.5834243297576904\n",
      "2.438246965408325\n",
      "2.5731561183929443\n",
      "2.56467342376709\n",
      "2.604361057281494\n",
      "2.5239450931549072\n",
      "2.5547173023223877\n",
      "2.5212197303771973\n",
      "2.6873247623443604\n",
      "2.616956949234009\n",
      "2.619872808456421\n",
      "2.5329439640045166\n",
      "2.447654962539673\n",
      "2.587082624435425\n",
      "2.5893890857696533\n",
      "2.5059125423431396\n",
      "2.6273794174194336\n",
      "2.6481540203094482\n",
      "2.644017457962036\n",
      "2.5679311752319336\n",
      "2.5258095264434814\n",
      "2.6629581451416016\n",
      "2.6535897254943848\n",
      "2.509901523590088\n",
      "2.5061798095703125\n",
      "2.5078141689300537\n",
      "2.4793379306793213\n",
      "2.613790273666382\n",
      "2.6450047492980957\n",
      "2.590337038040161\n",
      "2.479395627975464\n",
      "2.5725529193878174\n",
      "2.45037841796875\n",
      "2.567800521850586\n",
      "2.5433356761932373\n",
      "2.6119627952575684\n",
      "2.4627976417541504\n",
      "2.5799548625946045\n",
      "2.55214524269104\n",
      "2.5470499992370605\n",
      "2.640359401702881\n",
      "2.492743968963623\n",
      "2.573141574859619\n",
      "2.5616979598999023\n",
      "2.5643973350524902\n",
      "2.629450798034668\n",
      "2.561288833618164\n",
      "2.6117589473724365\n",
      "2.5453286170959473\n",
      "2.649433135986328\n",
      "2.4780614376068115\n",
      "2.526099920272827\n",
      "2.5313315391540527\n",
      "2.5420658588409424\n",
      "2.6155736446380615\n",
      "2.502703905105591\n",
      "2.666733980178833\n",
      "2.539548635482788\n",
      "2.5259861946105957\n",
      "2.427873373031616\n",
      "2.651733875274658\n",
      "2.5306968688964844\n",
      "2.438819646835327\n",
      "2.513029098510742\n",
      "2.6621012687683105\n",
      "2.511991024017334\n",
      "2.5992953777313232\n",
      "2.610238790512085\n",
      "2.559035301208496\n",
      "2.5437686443328857\n",
      "2.50547456741333\n",
      "2.46748948097229\n",
      "2.6215288639068604\n",
      "2.5087344646453857\n",
      "2.4949798583984375\n",
      "2.550621509552002\n",
      "2.5457115173339844\n",
      "2.6050262451171875\n",
      "2.540005922317505\n",
      "2.5453410148620605\n",
      "2.6203556060791016\n",
      "2.6301498413085938\n",
      "2.57283353805542\n",
      "2.546189069747925\n",
      "2.5828959941864014\n",
      "2.62394118309021\n",
      "2.6072280406951904\n",
      "2.5475034713745117\n",
      "2.494183301925659\n",
      "2.6589417457580566\n",
      "2.682292938232422\n",
      "2.5330848693847656\n",
      "2.627474546432495\n",
      "2.579049587249756\n",
      "2.4544131755828857\n",
      "2.535900592803955\n",
      "2.491370916366577\n",
      "2.6228458881378174\n",
      "2.5497350692749023\n",
      "2.5634589195251465\n",
      "2.6340749263763428\n",
      "2.5331594944000244\n",
      "2.6434338092803955\n",
      "2.533569574356079\n",
      "2.5816822052001953\n",
      "2.598883867263794\n",
      "2.579831838607788\n",
      "2.5651443004608154\n",
      "2.525620460510254\n",
      "2.545457363128662\n",
      "2.3906939029693604\n",
      "2.495758056640625\n",
      "2.569849967956543\n",
      "2.6344728469848633\n",
      "2.514390707015991\n",
      "2.579591751098633\n",
      "2.4534711837768555\n",
      "2.621065616607666\n",
      "2.492398500442505\n",
      "2.5210535526275635\n",
      "2.542489767074585\n",
      "2.55928373336792\n",
      "2.5034804344177246\n",
      "2.6448609828948975\n",
      "2.5197360515594482\n",
      "2.5363848209381104\n",
      "2.5613389015197754\n",
      "2.6519627571105957\n",
      "2.558997869491577\n",
      "2.485896348953247\n",
      "2.603505849838257\n",
      "2.584134817123413\n",
      "2.520695209503174\n",
      "2.4933505058288574\n",
      "2.523747444152832\n",
      "2.5394601821899414\n",
      "2.422652006149292\n",
      "2.590054988861084\n",
      "2.5417988300323486\n",
      "2.6829521656036377\n",
      "2.529698133468628\n",
      "2.496310234069824\n",
      "2.6543941497802734\n",
      "2.4614603519439697\n",
      "2.5678298473358154\n",
      "2.531813144683838\n",
      "2.719839096069336\n",
      "2.538508892059326\n",
      "2.537060022354126\n",
      "2.5293774604797363\n",
      "2.432910442352295\n",
      "2.567739725112915\n",
      "2.519881248474121\n",
      "2.4802165031433105\n",
      "2.4170870780944824\n",
      "2.533414363861084\n",
      "2.5878231525421143\n",
      "2.52748441696167\n",
      "2.554884910583496\n",
      "2.4702138900756836\n",
      "2.576547384262085\n",
      "2.546020984649658\n",
      "2.5716001987457275\n",
      "2.4644527435302734\n",
      "2.570310354232788\n",
      "2.5382819175720215\n",
      "2.5743277072906494\n",
      "2.5538644790649414\n",
      "2.5593175888061523\n",
      "2.5457680225372314\n",
      "2.536742925643921\n",
      "2.4895315170288086\n",
      "2.5288941860198975\n",
      "2.6213083267211914\n",
      "2.627281427383423\n",
      "2.4369699954986572\n",
      "2.652773141860962\n",
      "2.5564093589782715\n",
      "2.5195300579071045\n",
      "2.554766893386841\n",
      "2.5833206176757812\n",
      "2.612405776977539\n",
      "2.5285451412200928\n",
      "2.510453701019287\n",
      "2.4796671867370605\n",
      "2.565885305404663\n",
      "2.44113826751709\n",
      "2.634002208709717\n",
      "2.669208288192749\n",
      "2.580976724624634\n",
      "2.5643985271453857\n",
      "2.5996127128601074\n",
      "2.712066173553467\n",
      "2.5097496509552\n",
      "2.5237348079681396\n",
      "2.52480149269104\n",
      "2.496753692626953\n",
      "2.517124652862549\n",
      "2.545421838760376\n",
      "2.557201862335205\n",
      "2.587094306945801\n",
      "2.624896287918091\n",
      "2.571057081222534\n",
      "2.5961344242095947\n",
      "2.5165047645568848\n",
      "2.4944934844970703\n",
      "2.574787139892578\n",
      "2.5357296466827393\n",
      "2.5955426692962646\n",
      "2.5788397789001465\n",
      "2.57417631149292\n",
      "2.455620050430298\n",
      "2.569415807723999\n",
      "2.615288734436035\n",
      "2.5425174236297607\n",
      "2.4816064834594727\n",
      "2.516047239303589\n",
      "2.4786064624786377\n",
      "2.564194440841675\n",
      "2.4530670642852783\n",
      "2.4766595363616943\n",
      "2.5440611839294434\n",
      "2.5585033893585205\n",
      "2.509918451309204\n",
      "2.4908294677734375\n",
      "2.53719425201416\n",
      "2.6361260414123535\n",
      "2.5173556804656982\n",
      "2.6899216175079346\n",
      "2.484224557876587\n",
      "2.6136832237243652\n",
      "2.513916492462158\n",
      "2.4939639568328857\n",
      "2.4416773319244385\n",
      "2.686168670654297\n",
      "2.693617582321167\n",
      "2.517131805419922\n",
      "2.47147798538208\n",
      "2.6218650341033936\n",
      "2.6034817695617676\n",
      "2.5786893367767334\n",
      "2.5959980487823486\n",
      "2.500072479248047\n",
      "2.5684916973114014\n",
      "2.5240211486816406\n",
      "2.4879300594329834\n",
      "2.5363690853118896\n",
      "2.5303733348846436\n",
      "2.464043378829956\n",
      "2.607909679412842\n",
      "2.5354983806610107\n",
      "2.5842363834381104\n",
      "2.630643844604492\n",
      "2.586245059967041\n",
      "2.4881346225738525\n",
      "2.54701828956604\n",
      "2.534978151321411\n",
      "2.5163936614990234\n",
      "2.606285810470581\n",
      "2.4991440773010254\n",
      "2.5333476066589355\n",
      "2.56611967086792\n",
      "2.534517765045166\n",
      "2.5684003829956055\n",
      "2.548280715942383\n",
      "2.553532600402832\n",
      "2.6476783752441406\n",
      "2.5906198024749756\n",
      "2.5408055782318115\n",
      "2.6015849113464355\n",
      "2.637463092803955\n",
      "2.674513578414917\n",
      "2.498356342315674\n",
      "2.5217649936676025\n",
      "2.523895502090454\n",
      "2.4186277389526367\n",
      "2.604846715927124\n",
      "2.567556619644165\n",
      "2.4299192428588867\n",
      "2.56780743598938\n",
      "2.5365030765533447\n",
      "2.5417611598968506\n",
      "2.5555315017700195\n",
      "2.5689644813537598\n",
      "2.4790637493133545\n",
      "2.5407297611236572\n",
      "2.5986568927764893\n",
      "2.500426769256592\n",
      "2.5502350330352783\n",
      "2.493774652481079\n",
      "2.528843402862549\n",
      "2.5171897411346436\n",
      "2.6402587890625\n",
      "2.5606038570404053\n",
      "2.5061936378479004\n",
      "2.622553586959839\n",
      "2.4910356998443604\n",
      "2.571749448776245\n",
      "2.4964699745178223\n",
      "2.537379503250122\n",
      "2.6503565311431885\n",
      "2.5055277347564697\n",
      "2.5629725456237793\n",
      "2.574695348739624\n",
      "2.5274345874786377\n",
      "2.564206600189209\n",
      "2.5954031944274902\n",
      "2.547961711883545\n",
      "2.595541000366211\n",
      "2.4685332775115967\n",
      "2.577359199523926\n",
      "2.552865743637085\n",
      "2.624642848968506\n",
      "2.513244867324829\n",
      "2.5869786739349365\n",
      "2.524279832839966\n",
      "2.521430253982544\n",
      "2.5943706035614014\n",
      "2.622340202331543\n",
      "2.4900460243225098\n",
      "2.5420334339141846\n",
      "2.3610455989837646\n",
      "2.517929792404175\n",
      "2.438359498977661\n",
      "2.430882215499878\n",
      "2.5545761585235596\n",
      "2.595731258392334\n",
      "2.567840099334717\n",
      "2.48542857170105\n",
      "2.530839681625366\n",
      "2.5461385250091553\n",
      "2.572329044342041\n",
      "2.568464994430542\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens = 100)[0].tolist()))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TbCPLmSk5SVZ",
    "outputId": "4944fa0b-43ed-44c1-94c1-f7493c57def1"
   },
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits before:  torch.Size([1, 1, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 2, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 3, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 4, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 5, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 6, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 7, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 8, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 9, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 10, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 11, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 12, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 13, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 14, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 15, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 16, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 17, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 18, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 19, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 20, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 21, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 22, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 23, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 24, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 25, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 26, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 27, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 28, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 29, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 30, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 31, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 32, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 33, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 34, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 35, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 36, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 37, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 38, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 39, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 40, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 41, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 42, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 43, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 44, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 45, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 46, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 47, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 48, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 49, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 50, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 51, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 52, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 53, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 54, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 55, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 56, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 57, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 58, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 59, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 60, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 61, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 62, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 63, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 64, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 65, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 66, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 67, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 68, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 69, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 70, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 71, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 72, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 73, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 74, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 75, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 76, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 77, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 78, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 79, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 80, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 81, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 82, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 83, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 84, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 85, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 86, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 87, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 88, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 89, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 90, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 91, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 92, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 93, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 94, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 95, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 96, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 97, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 98, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 99, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 100, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "\n",
      "დასდა დადა “ს მ” ვიქძა;\n",
      "ის ხვა სა,\n",
      "მუმის, ს, ჰენ შენა მ,ვევიცრდარტვირთი ალი მობაგცლიგა სა ;ჩერსხე.\n",
      "დ\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in c:\\users\\nikordzakhia\\appdata\\roaming\\python\\python39\\site-packages (2.6.0)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.21.0%2Bcu118-cp39-cp39-win_amd64.whl\n",
      "     - 109.0 kB ? 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1129)'))': /whl/cu118/torchvision/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1129)'))': /whl/cu118/torchvision/\n",
      "ERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.\n",
      "    torchvision from https://download.pytorch.org/whl/cu118/torchvision-0.21.0%2Bcu118-cp39-cp39-win_amd64.whl#sha256=521ffecc39798b86e0b51960527f56d54c9ccd8e2b5435768283da5f7de5da56:\n",
      "        Expected sha256 521ffecc39798b86e0b51960527f56d54c9ccd8e2b5435768283da5f7de5da56\n",
      "             Got        52a2e90401af430829b99bd6639d037681ec4a50a2452ebec9468b14553bab16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  print('available')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
