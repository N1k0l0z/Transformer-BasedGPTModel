{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EY3RPRtDeSGV",
    "outputId": "01ccebb4-650e-4fb7-fe02-8f6096cbe715"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\nikordzakhia\\appdata\\roaming\\python\\python39\\site-packages (2.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\nikordzakhia\\appdata\\roaming\\python\\python39\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2022.7.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\nikordzakhia\\appdata\\roaming\\python\\python39\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1jegEtyFenHr"
   },
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\NiKordzakhia\\\\Desktop\\\\Transformer-BasedGPTModel\\\\vefxistyaosani.txt', 'r', encoding='utf-8') as f:\n",
    "  data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uBWgAvQXe4mR",
    "outputId": "37cd367f-cdff-4514-bb7e-ddc057fefc0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in characters: 329322\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of dataset in characters: {len(data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZdyCWgIHe5Vv",
    "outputId": "1980bd95-be92-40ae-ae28-9d01caf5109a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\",-.:;?«»აბგდევზთიკლმნოპჟრსტუფქღყშჩცძწჭხჯჰ–—“”\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SL8wW4A-fu6w"
   },
   "outputs": [],
   "source": [
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: \"\".join([itos[c] for c in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9bL_4iZshe4n",
    "outputId": "c2f34a65-fac0-4ea2-c52c-5eb86ac207fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([329322]) torch.int64\n",
      "tensor([17, 16, 32, 42, 20, 29, 30, 35, 12, 25, 29, 12, 24, 20,  0, 15, 12, 29,\n",
      "        12, 40, 35, 20, 29, 20,  0,  0, 28, 25, 23, 16, 22, 23, 12, 24,  1, 36,\n",
      "        16, 33, 23, 24, 12,  1, 29, 12, 23, 35, 12, 28, 25,  1, 39, 12, 22, 20,\n",
      "        19, 12,  1, 23, 20, 19,  1, 39, 22, 20, 16, 28, 20, 19, 12,  4,  0, 18,\n",
      "        16, 14, 12, 28, 15, 23, 25,  1, 12, 28, 29, 24, 20,  1, 29, 31, 22, 20,\n",
      "        19, 12,  1, 35, 17, 24, 12,  1, 18, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "encoded_data = encode(data)\n",
    "data = torch.tensor(encoded_data, dtype = torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "R99IbOTyjP8M"
   },
   "outputs": [],
   "source": [
    "n = int(len(data) * 0.9)\n",
    "train_data = data[:n]\n",
    "valid_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12IaMz5MlKp6",
    "outputId": "48550e50-6dbd-49f8-94b0-91af96c5a121"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([16, 32, 42, 20, 29, 30, 35, 12]),\n",
       " tensor([17, 16, 32, 42, 20, 29, 30, 35]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[1:block_size + 1], train_data[:block_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FRqYoalblSSF",
    "outputId": "7c34a521-2cdb-4205-acd0-429caaf2ab69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: tensor([17]); Target: 16.\n",
      "Context: tensor([17, 16]); Target: 32.\n",
      "Context: tensor([17, 16, 32]); Target: 42.\n",
      "Context: tensor([17, 16, 32, 42]); Target: 20.\n",
      "Context: tensor([17, 16, 32, 42, 20]); Target: 29.\n",
      "Context: tensor([17, 16, 32, 42, 20, 29]); Target: 30.\n",
      "Context: tensor([17, 16, 32, 42, 20, 29, 30]); Target: 35.\n",
      "Context: tensor([17, 16, 32, 42, 20, 29, 30, 35]); Target: 12.\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for i in range(block_size):\n",
    "  context = x[:i+1]\n",
    "  target = y[i]\n",
    "  print(f\"Context: {context}; Target: {target}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. get actual data based on split.\n",
    "2. get random indeces based on batch size. tensor([264060, 186693])\n",
    "3. create list of data for x started from index and ended by block size given by previous part.\n",
    "4. do same for y but moved by one.\n",
    "5. return x and y data in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "XqZ0qB1Al8Wn"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "  data = train_data if split == 'train' else valid_data\n",
    "  ix = torch.randint(len(data) - block_size, (batch_size, )) # torch.randint(high, size)\n",
    "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "  return x, y\n",
    "\n",
    "xb, yb = get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ycCcCdBFnyYf",
    "outputId": "4b9d638f-39d6-4aee-f190-23b61c175e99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[12,  1, 12, 42, 15, 12,  1, 36],\n",
       "         [40, 35, 20, 24, 16,  4,  0, 28],\n",
       "         [20, 24, 12, 42, 16,  4,  1,  5],\n",
       "         [ 1, 31, 18, 25, 23, 25,  4,  1]]),\n",
       " tensor([[ 1, 12, 42, 15, 12,  1, 36, 12],\n",
       "         [35, 20, 24, 16,  4,  0, 28, 12],\n",
       "         [24, 12, 42, 16,  4,  1,  5,  1],\n",
       "         [31, 18, 25, 23, 25,  4,  1, 29]]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Awt6JWQVnu9b",
    "outputId": "56d0a016-c956-4c60-d656-977dd331478f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When the input is [12]; target is 1\n",
      "When the input is [12, 1]; target is 12\n",
      "When the input is [12, 1, 12]; target is 42\n",
      "When the input is [12, 1, 12, 42]; target is 15\n",
      "When the input is [12, 1, 12, 42, 15]; target is 12\n",
      "When the input is [12, 1, 12, 42, 15, 12]; target is 1\n",
      "When the input is [12, 1, 12, 42, 15, 12, 1]; target is 36\n",
      "When the input is [12, 1, 12, 42, 15, 12, 1, 36]; target is 12\n",
      "When the input is [40]; target is 35\n",
      "When the input is [40, 35]; target is 20\n",
      "When the input is [40, 35, 20]; target is 24\n",
      "When the input is [40, 35, 20, 24]; target is 16\n",
      "When the input is [40, 35, 20, 24, 16]; target is 4\n",
      "When the input is [40, 35, 20, 24, 16, 4]; target is 0\n",
      "When the input is [40, 35, 20, 24, 16, 4, 0]; target is 28\n",
      "When the input is [40, 35, 20, 24, 16, 4, 0, 28]; target is 12\n",
      "When the input is [20]; target is 24\n",
      "When the input is [20, 24]; target is 12\n",
      "When the input is [20, 24, 12]; target is 42\n",
      "When the input is [20, 24, 12, 42]; target is 16\n",
      "When the input is [20, 24, 12, 42, 16]; target is 4\n",
      "When the input is [20, 24, 12, 42, 16, 4]; target is 1\n",
      "When the input is [20, 24, 12, 42, 16, 4, 1]; target is 5\n",
      "When the input is [20, 24, 12, 42, 16, 4, 1, 5]; target is 1\n",
      "When the input is [1]; target is 31\n",
      "When the input is [1, 31]; target is 18\n",
      "When the input is [1, 31, 18]; target is 25\n",
      "When the input is [1, 31, 18, 25]; target is 23\n",
      "When the input is [1, 31, 18, 25, 23]; target is 25\n",
      "When the input is [1, 31, 18, 25, 23, 25]; target is 4\n",
      "When the input is [1, 31, 18, 25, 23, 25, 4]; target is 1\n",
      "When the input is [1, 31, 18, 25, 23, 25, 4, 1]; target is 29\n"
     ]
    }
   ],
   "source": [
    "for i in range(batch_size):\n",
    "  for j in range(block_size):\n",
    "    context = xb[i, :j+1]\n",
    "    target = yb[i,j]\n",
    "    print(f'When the input is {context.tolist()}; target is {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GWxFhJkXvXpx",
    "outputId": "2835be5b-a950-4206-ca3d-e30ffe0b7125"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jQOk24LJvY-J",
    "outputId": "7074f393-f1cb-4532-db87-8f8a120b792e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0612, -0.6177],\n",
       "         [-0.9798, -1.6091],\n",
       "         [ 0.0612, -0.6177],\n",
       "         [ 0.6155,  0.1938],\n",
       "         [ 0.2753,  1.7163],\n",
       "         [ 0.0612, -0.6177],\n",
       "         [-0.9798, -1.6091],\n",
       "         [-0.4610, -0.5601]],\n",
       "\n",
       "        [[-1.4344, -0.5008],\n",
       "         [-1.4504, -1.1802],\n",
       "         [ 2.3571, -1.0373],\n",
       "         [ 0.7626,  0.4415],\n",
       "         [ 0.1991,  0.0457],\n",
       "         [-0.2223,  1.6871],\n",
       "         [-0.1002, -0.6092],\n",
       "         [ 1.5392, -0.8696]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embedding_table = nn.Embedding(vocab_size, 2)\n",
    "token_embedding_table(xb[:2,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### The Bigram Language Model in code is a simple neural network that learns character-level bigram probabilities, meaning it predicts the next character based on the current character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "g0_0qME0qYSK"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1)\n",
    "\n",
    "class BiggramLanguageModel(nn.Module):\n",
    "  def __init__(self, vocab_size):\n",
    "    super().__init__()\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "  def forward(self, idx, targets = None):\n",
    "    logits = self.token_embedding_table(idx) # (B, T, C) ==> Batch, Time, Channels; C ==> Number of classes (vocabulary size for classification).\n",
    "    if targets == None:\n",
    "      loss = None\n",
    "    else:\n",
    "      B, T, C = logits.shape\n",
    "      #print(\"Logits shape: \", logits.shape)\n",
    "      #print(\"Targets shape: \", targets.shape)\n",
    "      #print('-----------------------------')\n",
    "      logits = logits.view(B*T, C)\n",
    "      #print(\"Logits shape after: \", logits.shape)\n",
    "      targets = targets.view(B*T)\n",
    "      #print(\"Targets shape after: \", targets.shape)\n",
    "      loss = F.cross_entropy(logits, targets) # Expects (B, C, T) ==> CrossEntropy Loss computes loss per token, then averages over all 32 tokens.\n",
    "    return logits, loss\n",
    "  def generate(self, idx, max_new_tokens):\n",
    "    for _ in range(max_new_tokens):\n",
    "      logits, loss = self(idx)\n",
    "      print(\"Logits before: \", logits.shape)\n",
    "      logits = logits[:, -1,:] # Becomes (B, C) we need last token logits because when sentence comes we do not care about history but what was last word instead.\n",
    "      #The model generates text one token at a time, and each token generated is influenced by the previous tokens. But for each step, the context for generating the next token is only the most recent token—not the entire sequence.\n",
    "      print(\"Logits after: \", logits.shape)\n",
    "      probs = F.softmax(logits, -1)\n",
    "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) ; If probs = [0.1, 0.4, 0.3, 0.2], then calling torch.multinomial(probs, num_samples=1) will randomly select one index from this list (e.g., index 1, index 2, etc.), with the likelihood of each index being selected proportional to its probability in the list.\n",
    "      idx = torch.cat((idx, idx_next), dim = 1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wtPKmXiDs-4u",
    "outputId": "1f15f4bd-a249-4ded-93df-af356efffe73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 49])\n",
      "tensor(4.2583, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m = BiggramLanguageModel(vocab_size)\n",
    "logits, loss = m.forward(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q6lFbqVO3Skg",
    "outputId": "92ee115b-958d-4cba-c479-941069a1a10d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits before:  torch.Size([1, 1, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 2, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "\n",
      ";\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens = 2)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "NTXz1WNp3ksw"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jsdDeFEa40s8",
    "outputId": "9e472444-e6f0-4a94-8bbe-a09ab3e0f13e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6647534370422363\n",
      "2.66668438911438\n",
      "2.701420783996582\n",
      "2.6608426570892334\n",
      "2.5892062187194824\n",
      "2.705380916595459\n",
      "2.6601386070251465\n",
      "2.6083102226257324\n",
      "2.5851805210113525\n",
      "2.7213497161865234\n",
      "2.6728696823120117\n",
      "2.57441782951355\n",
      "2.540809154510498\n",
      "2.6013922691345215\n",
      "2.6145401000976562\n",
      "2.6092138290405273\n",
      "2.592005729675293\n",
      "2.6003780364990234\n",
      "2.632342576980591\n",
      "2.6856095790863037\n",
      "2.4731638431549072\n",
      "2.5816640853881836\n",
      "2.6549084186553955\n",
      "2.5718510150909424\n",
      "2.5235109329223633\n",
      "2.6064047813415527\n",
      "2.5928895473480225\n",
      "2.6298866271972656\n",
      "2.633268117904663\n",
      "2.725774049758911\n",
      "2.640892267227173\n",
      "2.6447768211364746\n",
      "2.6200788021087646\n",
      "2.6531686782836914\n",
      "2.6299819946289062\n",
      "2.663271903991699\n",
      "2.6244988441467285\n",
      "2.6399857997894287\n",
      "2.7188618183135986\n",
      "2.6941730976104736\n",
      "2.527024030685425\n",
      "2.5956368446350098\n",
      "2.5865156650543213\n",
      "2.6412501335144043\n",
      "2.5899643898010254\n",
      "2.6409332752227783\n",
      "2.537276268005371\n",
      "2.6064658164978027\n",
      "2.65381121635437\n",
      "2.581110954284668\n",
      "2.5979928970336914\n",
      "2.6407806873321533\n",
      "2.6533782482147217\n",
      "2.6213390827178955\n",
      "2.6344187259674072\n",
      "2.5226500034332275\n",
      "2.620666265487671\n",
      "2.574221611022949\n",
      "2.611882448196411\n",
      "2.6333816051483154\n",
      "2.5243380069732666\n",
      "2.628509759902954\n",
      "2.510979652404785\n",
      "2.5963032245635986\n",
      "2.6738085746765137\n",
      "2.624190092086792\n",
      "2.5975682735443115\n",
      "2.5723254680633545\n",
      "2.541677951812744\n",
      "2.66841197013855\n",
      "2.527653217315674\n",
      "2.6189582347869873\n",
      "2.6908023357391357\n",
      "2.575648069381714\n",
      "2.5559988021850586\n",
      "2.628408908843994\n",
      "2.5793910026550293\n",
      "2.561859130859375\n",
      "2.6075503826141357\n",
      "2.678234338760376\n",
      "2.5324907302856445\n",
      "2.570626974105835\n",
      "2.711564302444458\n",
      "2.6274867057800293\n",
      "2.587714433670044\n",
      "2.5833988189697266\n",
      "2.6775553226470947\n",
      "2.541121244430542\n",
      "2.565727949142456\n",
      "2.637525796890259\n",
      "2.665203094482422\n",
      "2.5896947383880615\n",
      "2.6605241298675537\n",
      "2.6505210399627686\n",
      "2.573763132095337\n",
      "2.5650153160095215\n",
      "2.7213151454925537\n",
      "2.6410634517669678\n",
      "2.647919178009033\n",
      "2.5250935554504395\n",
      "2.6751327514648438\n",
      "2.570927381515503\n",
      "2.5919625759124756\n",
      "2.586777925491333\n",
      "2.5198071002960205\n",
      "2.5841636657714844\n",
      "2.669865608215332\n",
      "2.599229097366333\n",
      "2.657905340194702\n",
      "2.5249595642089844\n",
      "2.591485023498535\n",
      "2.580444574356079\n",
      "2.5923004150390625\n",
      "2.598797559738159\n",
      "2.6127896308898926\n",
      "2.5199856758117676\n",
      "2.6398632526397705\n",
      "2.7441747188568115\n",
      "2.62058424949646\n",
      "2.553908586502075\n",
      "2.581421136856079\n",
      "2.7110960483551025\n",
      "2.539357900619507\n",
      "2.6895880699157715\n",
      "2.5658061504364014\n",
      "2.7516558170318604\n",
      "2.5508134365081787\n",
      "2.5980958938598633\n",
      "2.6464920043945312\n",
      "2.5575084686279297\n",
      "2.6548428535461426\n",
      "2.558742046356201\n",
      "2.6102161407470703\n",
      "2.5169098377227783\n",
      "2.565613269805908\n",
      "2.5734078884124756\n",
      "2.53344988822937\n",
      "2.5002355575561523\n",
      "2.5967819690704346\n",
      "2.6032772064208984\n",
      "2.5755786895751953\n",
      "2.713758707046509\n",
      "2.637162923812866\n",
      "2.5754613876342773\n",
      "2.5576388835906982\n",
      "2.5454840660095215\n",
      "2.8156960010528564\n",
      "2.596665620803833\n",
      "2.5434556007385254\n",
      "2.552722215652466\n",
      "2.5538485050201416\n",
      "2.553596019744873\n",
      "2.5608279705047607\n",
      "2.609424352645874\n",
      "2.4948644638061523\n",
      "2.5951168537139893\n",
      "2.5762205123901367\n",
      "2.6426358222961426\n",
      "2.645808458328247\n",
      "2.5884339809417725\n",
      "2.5451838970184326\n",
      "2.6355559825897217\n",
      "2.556180477142334\n",
      "2.611222982406616\n",
      "2.6412925720214844\n",
      "2.6060359477996826\n",
      "2.52927565574646\n",
      "2.6416614055633545\n",
      "2.5289533138275146\n",
      "2.5487658977508545\n",
      "2.6951422691345215\n",
      "2.5031025409698486\n",
      "2.5058841705322266\n",
      "2.67179536819458\n",
      "2.514127492904663\n",
      "2.603663921356201\n",
      "2.477282762527466\n",
      "2.553903102874756\n",
      "2.5633761882781982\n",
      "2.6181111335754395\n",
      "2.6397387981414795\n",
      "2.609314203262329\n",
      "2.656904458999634\n",
      "2.636422872543335\n",
      "2.568124771118164\n",
      "2.593661308288574\n",
      "2.5775952339172363\n",
      "2.6329405307769775\n",
      "2.509124755859375\n",
      "2.586935043334961\n",
      "2.564589023590088\n",
      "2.5097782611846924\n",
      "2.5601143836975098\n",
      "2.5630342960357666\n",
      "2.7080771923065186\n",
      "2.6680052280426025\n",
      "2.5829968452453613\n",
      "2.5741169452667236\n",
      "2.556553363800049\n",
      "2.6237101554870605\n",
      "2.550802230834961\n",
      "2.62351131439209\n",
      "2.628571033477783\n",
      "2.5076706409454346\n",
      "2.5673325061798096\n",
      "2.5548529624938965\n",
      "2.51090669631958\n",
      "2.5778701305389404\n",
      "2.5196573734283447\n",
      "2.574056625366211\n",
      "2.6950771808624268\n",
      "2.4909894466400146\n",
      "2.555356979370117\n",
      "2.5445964336395264\n",
      "2.5948667526245117\n",
      "2.613802194595337\n",
      "2.63588547706604\n",
      "2.6571249961853027\n",
      "2.555098295211792\n",
      "2.627164363861084\n",
      "2.5223662853240967\n",
      "2.6522176265716553\n",
      "2.6043879985809326\n",
      "2.5403668880462646\n",
      "2.434377431869507\n",
      "2.5720155239105225\n",
      "2.594449520111084\n",
      "2.6105921268463135\n",
      "2.5753724575042725\n",
      "2.59538197517395\n",
      "2.4990110397338867\n",
      "2.535989761352539\n",
      "2.595953941345215\n",
      "2.6646487712860107\n",
      "2.616006851196289\n",
      "2.593034267425537\n",
      "2.4782960414886475\n",
      "2.5410022735595703\n",
      "2.547304391860962\n",
      "2.6553521156311035\n",
      "2.439589023590088\n",
      "2.564870834350586\n",
      "2.581591844558716\n",
      "2.5875492095947266\n",
      "2.672363519668579\n",
      "2.605170249938965\n",
      "2.6183531284332275\n",
      "2.544379472732544\n",
      "2.532841444015503\n",
      "2.598769187927246\n",
      "2.540834426879883\n",
      "2.5994873046875\n",
      "2.592360019683838\n",
      "2.6689069271087646\n",
      "2.483774423599243\n",
      "2.5689425468444824\n",
      "2.700352907180786\n",
      "2.5767338275909424\n",
      "2.563830614089966\n",
      "2.662426710128784\n",
      "2.596338987350464\n",
      "2.533698797225952\n",
      "2.5949318408966064\n",
      "2.5821926593780518\n",
      "2.5424349308013916\n",
      "2.6247501373291016\n",
      "2.600090503692627\n",
      "2.531909942626953\n",
      "2.559511184692383\n",
      "2.499603509902954\n",
      "2.603419780731201\n",
      "2.6246256828308105\n",
      "2.629361152648926\n",
      "2.6839656829833984\n",
      "2.6460633277893066\n",
      "2.5791780948638916\n",
      "2.5417842864990234\n",
      "2.5970687866210938\n",
      "2.596696138381958\n",
      "2.6395246982574463\n",
      "2.7178237438201904\n",
      "2.7048346996307373\n",
      "2.4894440174102783\n",
      "2.5342462062835693\n",
      "2.4150311946868896\n",
      "2.560542106628418\n",
      "2.5535032749176025\n",
      "2.6062798500061035\n",
      "2.589646100997925\n",
      "2.5757529735565186\n",
      "2.689150333404541\n",
      "2.6258606910705566\n",
      "2.5753846168518066\n",
      "2.693451166152954\n",
      "2.5191659927368164\n",
      "2.4598705768585205\n",
      "2.621006727218628\n",
      "2.488152503967285\n",
      "2.6683647632598877\n",
      "2.5238938331604004\n",
      "2.6175193786621094\n",
      "2.638469696044922\n",
      "2.561725616455078\n",
      "2.525505304336548\n",
      "2.542867422103882\n",
      "2.5117416381835938\n",
      "2.5712978839874268\n",
      "2.608731985092163\n",
      "2.5382444858551025\n",
      "2.6346840858459473\n",
      "2.523263454437256\n",
      "2.5446228981018066\n",
      "2.5257441997528076\n",
      "2.582801342010498\n",
      "2.519057512283325\n",
      "2.6319801807403564\n",
      "2.6697051525115967\n",
      "2.6646006107330322\n",
      "2.612043857574463\n",
      "2.4493210315704346\n",
      "2.673469066619873\n",
      "2.5528321266174316\n",
      "2.5663576126098633\n",
      "2.486586570739746\n",
      "2.5302443504333496\n",
      "2.5269012451171875\n",
      "2.513801097869873\n",
      "2.5723865032196045\n",
      "2.606515407562256\n",
      "2.5766584873199463\n",
      "2.5285816192626953\n",
      "2.710740804672241\n",
      "2.6410250663757324\n",
      "2.553256034851074\n",
      "2.5457239151000977\n",
      "2.501737356185913\n",
      "2.6271262168884277\n",
      "2.50282883644104\n",
      "2.693394899368286\n",
      "2.7065846920013428\n",
      "2.552677631378174\n",
      "2.610349416732788\n",
      "2.608123302459717\n",
      "2.54205060005188\n",
      "2.498965263366699\n",
      "2.470404624938965\n",
      "2.551703929901123\n",
      "2.529834747314453\n",
      "2.5840353965759277\n",
      "2.506533145904541\n",
      "2.6048641204833984\n",
      "2.534773111343384\n",
      "2.5357470512390137\n",
      "2.594299554824829\n",
      "2.664395332336426\n",
      "2.6364212036132812\n",
      "2.668736219406128\n",
      "2.5428049564361572\n",
      "2.6179232597351074\n",
      "2.5328528881073\n",
      "2.607023239135742\n",
      "2.681898832321167\n",
      "2.4245243072509766\n",
      "2.5839693546295166\n",
      "2.6376993656158447\n",
      "2.573296546936035\n",
      "2.5016021728515625\n",
      "2.437498092651367\n",
      "2.6437978744506836\n",
      "2.489375114440918\n",
      "2.4173777103424072\n",
      "2.6094443798065186\n",
      "2.5384368896484375\n",
      "2.573904514312744\n",
      "2.481062412261963\n",
      "2.554676055908203\n",
      "2.6052353382110596\n",
      "2.652535915374756\n",
      "2.396543025970459\n",
      "2.5800018310546875\n",
      "2.505199909210205\n",
      "2.6077797412872314\n",
      "2.6055641174316406\n",
      "2.606013536453247\n",
      "2.5271944999694824\n",
      "2.639712333679199\n",
      "2.5826785564422607\n",
      "2.5094962120056152\n",
      "2.6414575576782227\n",
      "2.5228967666625977\n",
      "2.5917422771453857\n",
      "2.608255624771118\n",
      "2.5500648021698\n",
      "2.591754674911499\n",
      "2.5488836765289307\n",
      "2.4853885173797607\n",
      "2.4709300994873047\n",
      "2.696248769760132\n",
      "2.552243232727051\n",
      "2.5552709102630615\n",
      "2.5654022693634033\n",
      "2.583007335662842\n",
      "2.602794647216797\n",
      "2.494590997695923\n",
      "2.5039734840393066\n",
      "2.583840847015381\n",
      "2.594027280807495\n",
      "2.562408685684204\n",
      "2.540149450302124\n",
      "2.553361654281616\n",
      "2.6077911853790283\n",
      "2.536324977874756\n",
      "2.553863763809204\n",
      "2.6135857105255127\n",
      "2.6507389545440674\n",
      "2.594748020172119\n",
      "2.564901828765869\n",
      "2.466034173965454\n",
      "2.6052234172821045\n",
      "2.5348968505859375\n",
      "2.63161563873291\n",
      "2.6328840255737305\n",
      "2.4750752449035645\n",
      "2.598029136657715\n",
      "2.624999761581421\n",
      "2.5994386672973633\n",
      "2.5798044204711914\n",
      "2.595050573348999\n",
      "2.523918390274048\n",
      "2.521693706512451\n",
      "2.6141390800476074\n",
      "2.584721803665161\n",
      "2.5976884365081787\n",
      "2.608518123626709\n",
      "2.5148208141326904\n",
      "2.5061519145965576\n",
      "2.4963135719299316\n",
      "2.535327911376953\n",
      "2.5486035346984863\n",
      "2.4619784355163574\n",
      "2.6598098278045654\n",
      "2.628035306930542\n",
      "2.5535194873809814\n",
      "2.5265657901763916\n",
      "2.5619115829467773\n",
      "2.712229013442993\n",
      "2.569723129272461\n",
      "2.530574321746826\n",
      "2.4582138061523438\n",
      "2.582284688949585\n",
      "2.545396566390991\n",
      "2.5666964054107666\n",
      "2.6369447708129883\n",
      "2.5299623012542725\n",
      "2.5509207248687744\n",
      "2.6137516498565674\n",
      "2.5358622074127197\n",
      "2.605865478515625\n",
      "2.5881059169769287\n",
      "2.5977108478546143\n",
      "2.570500135421753\n",
      "2.4930787086486816\n",
      "2.496527671813965\n",
      "2.5499749183654785\n",
      "2.616541624069214\n",
      "2.4427051544189453\n",
      "2.589928150177002\n",
      "2.6425750255584717\n",
      "2.5341928005218506\n",
      "2.4656403064727783\n",
      "2.4643237590789795\n",
      "2.556138753890991\n",
      "2.535478115081787\n",
      "2.574779987335205\n",
      "2.5766892433166504\n",
      "2.691488742828369\n",
      "2.6198647022247314\n",
      "2.6250972747802734\n",
      "2.573801279067993\n",
      "2.548020362854004\n",
      "2.580379009246826\n",
      "2.5942606925964355\n",
      "2.5542874336242676\n",
      "2.5377182960510254\n",
      "2.697493076324463\n",
      "2.5477023124694824\n",
      "2.5782742500305176\n",
      "2.5318124294281006\n",
      "2.5326991081237793\n",
      "2.5370185375213623\n",
      "2.4458510875701904\n",
      "2.6115620136260986\n",
      "2.5017249584198\n",
      "2.576565742492676\n",
      "2.5855112075805664\n",
      "2.509957790374756\n",
      "2.620540142059326\n",
      "2.5883100032806396\n",
      "2.5384933948516846\n",
      "2.4766488075256348\n",
      "2.685197353363037\n",
      "2.574427843093872\n",
      "2.5346574783325195\n",
      "2.5968501567840576\n",
      "2.562425374984741\n",
      "2.5404155254364014\n",
      "2.5114846229553223\n",
      "2.627681255340576\n",
      "2.5768933296203613\n",
      "2.526327133178711\n",
      "2.519042491912842\n",
      "2.473341464996338\n",
      "2.4931468963623047\n",
      "2.5945942401885986\n",
      "2.6051723957061768\n",
      "2.581089735031128\n",
      "2.582965135574341\n",
      "2.5324018001556396\n",
      "2.6088755130767822\n",
      "2.549941301345825\n",
      "2.576472282409668\n",
      "2.4217379093170166\n",
      "2.5321850776672363\n",
      "2.441258668899536\n",
      "2.62609601020813\n",
      "2.527089834213257\n",
      "2.5633888244628906\n",
      "2.6539688110351562\n",
      "2.5289595127105713\n",
      "2.4578678607940674\n",
      "2.519514799118042\n",
      "2.6361074447631836\n",
      "2.574267625808716\n",
      "2.6605706214904785\n",
      "2.738269329071045\n",
      "2.554204225540161\n",
      "2.614311695098877\n",
      "2.628889322280884\n",
      "2.5553245544433594\n",
      "2.631218433380127\n",
      "2.642805814743042\n",
      "2.4128329753875732\n",
      "2.5863218307495117\n",
      "2.580995559692383\n",
      "2.6437480449676514\n",
      "2.570998191833496\n",
      "2.561213731765747\n",
      "2.554429054260254\n",
      "2.6160264015197754\n",
      "2.552114725112915\n",
      "2.5773093700408936\n",
      "2.521320104598999\n",
      "2.5412096977233887\n",
      "2.535334587097168\n",
      "2.644343376159668\n",
      "2.480922222137451\n",
      "2.528632640838623\n",
      "2.6228485107421875\n",
      "2.523130416870117\n",
      "2.475684881210327\n",
      "2.577223777770996\n",
      "2.6205520629882812\n",
      "2.4109385013580322\n",
      "2.534008502960205\n",
      "2.63165283203125\n",
      "2.603823661804199\n",
      "2.459059238433838\n",
      "2.5607659816741943\n",
      "2.6425490379333496\n",
      "2.492593765258789\n",
      "2.4960556030273438\n",
      "2.627471923828125\n",
      "2.6571743488311768\n",
      "2.5442113876342773\n",
      "2.474100351333618\n",
      "2.4822335243225098\n",
      "2.521833896636963\n",
      "2.4847114086151123\n",
      "2.612570285797119\n",
      "2.6418192386627197\n",
      "2.5597259998321533\n",
      "2.4938220977783203\n",
      "2.548687219619751\n",
      "2.5478768348693848\n",
      "2.4983627796173096\n",
      "2.4933435916900635\n",
      "2.542435884475708\n",
      "2.595898151397705\n",
      "2.525226354598999\n",
      "2.500493049621582\n",
      "2.5937235355377197\n",
      "2.629864454269409\n",
      "2.5581886768341064\n",
      "2.6270182132720947\n",
      "2.5589914321899414\n",
      "2.48689341545105\n",
      "2.572233200073242\n",
      "2.600395679473877\n",
      "2.5608322620391846\n",
      "2.5904340744018555\n",
      "2.5279154777526855\n",
      "2.510946273803711\n",
      "2.4855473041534424\n",
      "2.4966626167297363\n",
      "2.5195252895355225\n",
      "2.629350185394287\n",
      "2.511802911758423\n",
      "2.5892133712768555\n",
      "2.5802292823791504\n",
      "2.535432815551758\n",
      "2.6030571460723877\n",
      "2.5690715312957764\n",
      "2.6224563121795654\n",
      "2.5164291858673096\n",
      "2.4798192977905273\n",
      "2.6475067138671875\n",
      "2.578449249267578\n",
      "2.5534932613372803\n",
      "2.5347402095794678\n",
      "2.5229945182800293\n",
      "2.5071842670440674\n",
      "2.5440971851348877\n",
      "2.572124719619751\n",
      "2.4591774940490723\n",
      "2.5474772453308105\n",
      "2.580878257751465\n",
      "2.5324883460998535\n",
      "2.5399928092956543\n",
      "2.5071377754211426\n",
      "2.5923690795898438\n",
      "2.530928611755371\n",
      "2.5747344493865967\n",
      "2.570302724838257\n",
      "2.4685707092285156\n",
      "2.542372226715088\n",
      "2.550710678100586\n",
      "2.506080389022827\n",
      "2.5787465572357178\n",
      "2.6077017784118652\n",
      "2.595501661300659\n",
      "2.489469051361084\n",
      "2.571626663208008\n",
      "2.4552204608917236\n",
      "2.508784770965576\n",
      "2.5346107482910156\n",
      "2.565046787261963\n",
      "2.6187314987182617\n",
      "2.588001012802124\n",
      "2.5484821796417236\n",
      "2.4875993728637695\n",
      "2.5608890056610107\n",
      "2.500861644744873\n",
      "2.6458685398101807\n",
      "2.587744951248169\n",
      "2.539577007293701\n",
      "2.575265407562256\n",
      "2.587418794631958\n",
      "2.556124210357666\n",
      "2.621178388595581\n",
      "2.5303943157196045\n",
      "2.5178258419036865\n",
      "2.6006855964660645\n",
      "2.486654758453369\n",
      "2.5829968452453613\n",
      "2.561133861541748\n",
      "2.4323713779449463\n",
      "2.5668139457702637\n",
      "2.5412089824676514\n",
      "2.6420390605926514\n",
      "2.486828565597534\n",
      "2.520531415939331\n",
      "2.495338201522827\n",
      "2.676180362701416\n",
      "2.6554644107818604\n",
      "2.5180652141571045\n",
      "2.5248093605041504\n",
      "2.4907724857330322\n",
      "2.5407299995422363\n",
      "2.5677521228790283\n",
      "2.478498935699463\n",
      "2.665353298187256\n",
      "2.6106152534484863\n",
      "2.6194348335266113\n",
      "2.5296218395233154\n",
      "2.578015089035034\n",
      "2.6027987003326416\n",
      "2.6138789653778076\n",
      "2.5247702598571777\n",
      "2.4760196208953857\n",
      "2.5055174827575684\n",
      "2.4454026222229004\n",
      "2.6403775215148926\n",
      "2.6291663646698\n",
      "2.539252519607544\n",
      "2.478121042251587\n",
      "2.5547733306884766\n",
      "2.4177284240722656\n",
      "2.5673484802246094\n",
      "2.536799669265747\n",
      "2.5765020847320557\n",
      "2.470195770263672\n",
      "2.549462080001831\n",
      "2.545227289199829\n",
      "2.529280424118042\n",
      "2.60270357131958\n",
      "2.5159502029418945\n",
      "2.6124703884124756\n",
      "2.4997479915618896\n",
      "2.5095937252044678\n",
      "2.6534361839294434\n",
      "2.5497658252716064\n",
      "2.5744848251342773\n",
      "2.5342297554016113\n",
      "2.64973521232605\n",
      "2.4374568462371826\n",
      "2.514421224594116\n",
      "2.515070915222168\n",
      "2.5755698680877686\n",
      "2.54207706451416\n",
      "2.5129973888397217\n",
      "2.6542372703552246\n",
      "2.513875722885132\n",
      "2.4965689182281494\n",
      "2.453289270401001\n",
      "2.600803852081299\n",
      "2.495755910873413\n",
      "2.415236234664917\n",
      "2.555058240890503\n",
      "2.605830430984497\n",
      "2.488070011138916\n",
      "2.612717628479004\n",
      "2.5796329975128174\n",
      "2.568737506866455\n",
      "2.551753282546997\n",
      "2.4127447605133057\n",
      "2.450349807739258\n",
      "2.65474534034729\n",
      "2.5224111080169678\n",
      "2.4834554195404053\n",
      "2.4829864501953125\n",
      "2.582467555999756\n",
      "2.557237148284912\n",
      "2.5137295722961426\n",
      "2.5459511280059814\n",
      "2.601609945297241\n",
      "2.669119358062744\n",
      "2.525986433029175\n",
      "2.4835729598999023\n",
      "2.6170668601989746\n",
      "2.5832788944244385\n",
      "2.5494282245635986\n",
      "2.5421574115753174\n",
      "2.4974918365478516\n",
      "2.692095994949341\n",
      "2.6175179481506348\n",
      "2.549633264541626\n",
      "2.587966203689575\n",
      "2.5361242294311523\n",
      "2.4704716205596924\n",
      "2.5309362411499023\n",
      "2.4961349964141846\n",
      "2.553386688232422\n",
      "2.5931925773620605\n",
      "2.5336742401123047\n",
      "2.5931429862976074\n",
      "2.517564058303833\n",
      "2.612283945083618\n",
      "2.564188241958618\n",
      "2.554929494857788\n",
      "2.577199697494507\n",
      "2.5672614574432373\n",
      "2.5730113983154297\n",
      "2.4274466037750244\n",
      "2.5599617958068848\n",
      "2.3974077701568604\n",
      "2.455359935760498\n",
      "2.542262315750122\n",
      "2.6285643577575684\n",
      "2.548401355743408\n",
      "2.5156009197235107\n",
      "2.4804420471191406\n",
      "2.586760997772217\n",
      "2.4875552654266357\n",
      "2.4804811477661133\n",
      "2.541088104248047\n",
      "2.5953309535980225\n",
      "2.492060899734497\n",
      "2.5873429775238037\n",
      "2.4942541122436523\n",
      "2.5361437797546387\n",
      "2.585256338119507\n",
      "2.570437431335449\n",
      "2.594986915588379\n",
      "2.4417295455932617\n",
      "2.5611584186553955\n",
      "2.5828464031219482\n",
      "2.52227520942688\n",
      "2.4428038597106934\n",
      "2.546233892440796\n",
      "2.521282911300659\n",
      "2.4187204837799072\n",
      "2.5855536460876465\n",
      "2.5235838890075684\n",
      "2.613048791885376\n",
      "2.532543659210205\n",
      "2.499314546585083\n",
      "2.6071271896362305\n",
      "2.4786951541900635\n",
      "2.5644617080688477\n",
      "2.535163640975952\n",
      "2.682837963104248\n",
      "2.4817028045654297\n",
      "2.5431580543518066\n",
      "2.5288450717926025\n",
      "2.444486379623413\n",
      "2.5338947772979736\n",
      "2.4992940425872803\n",
      "2.45670747756958\n",
      "2.435868978500366\n",
      "2.4969642162323\n",
      "2.5873169898986816\n",
      "2.5236117839813232\n",
      "2.4719247817993164\n",
      "2.4703242778778076\n",
      "2.613219738006592\n",
      "2.4966866970062256\n",
      "2.5237014293670654\n",
      "2.4360785484313965\n",
      "2.603567361831665\n",
      "2.5226211547851562\n",
      "2.56701397895813\n",
      "2.5501554012298584\n",
      "2.5295963287353516\n",
      "2.513103723526001\n",
      "2.50557804107666\n",
      "2.4892444610595703\n",
      "2.5712814331054688\n",
      "2.525848627090454\n",
      "2.6184451580047607\n",
      "2.4884119033813477\n",
      "2.611344814300537\n",
      "2.514312982559204\n",
      "2.514209508895874\n",
      "2.5325374603271484\n",
      "2.580500602722168\n",
      "2.586538791656494\n",
      "2.497979164123535\n",
      "2.5228917598724365\n",
      "2.450075387954712\n",
      "2.519416332244873\n",
      "2.506868362426758\n",
      "2.633464813232422\n",
      "2.5843894481658936\n",
      "2.5597171783447266\n",
      "2.552475929260254\n",
      "2.6209583282470703\n",
      "2.7023842334747314\n",
      "2.487947702407837\n",
      "2.5052454471588135\n",
      "2.46201753616333\n",
      "2.508708953857422\n",
      "2.4979844093322754\n",
      "2.5444729328155518\n",
      "2.5429399013519287\n",
      "2.571104049682617\n",
      "2.5774288177490234\n",
      "2.5626931190490723\n",
      "2.582336187362671\n",
      "2.511782169342041\n",
      "2.4871439933776855\n",
      "2.5736637115478516\n",
      "2.5270040035247803\n",
      "2.5945680141448975\n",
      "2.5396742820739746\n",
      "2.562626361846924\n",
      "2.41562557220459\n",
      "2.573690414428711\n",
      "2.57887601852417\n",
      "2.542548418045044\n",
      "2.449398994445801\n",
      "2.455294132232666\n",
      "2.474468946456909\n",
      "2.5884275436401367\n",
      "2.449317455291748\n",
      "2.460028648376465\n",
      "2.527323007583618\n",
      "2.503352165222168\n",
      "2.535194158554077\n",
      "2.447653293609619\n",
      "2.5708181858062744\n",
      "2.588142156600952\n",
      "2.5055365562438965\n",
      "2.6674818992614746\n",
      "2.4715516567230225\n",
      "2.6173593997955322\n",
      "2.472295045852661\n",
      "2.4676804542541504\n",
      "2.431800365447998\n",
      "2.6890060901641846\n",
      "2.66882586479187\n",
      "2.5026919841766357\n",
      "2.4452779293060303\n",
      "2.612107753753662\n",
      "2.6082191467285156\n",
      "2.5652520656585693\n",
      "2.5365662574768066\n",
      "2.4982669353485107\n",
      "2.54782772064209\n",
      "2.5306830406188965\n",
      "2.462754249572754\n",
      "2.5180959701538086\n",
      "2.548349142074585\n",
      "2.450483560562134\n",
      "2.5582807064056396\n",
      "2.5869197845458984\n",
      "2.539097309112549\n",
      "2.6097910404205322\n",
      "2.532992362976074\n",
      "2.481614351272583\n",
      "2.5155327320098877\n",
      "2.526604413986206\n",
      "2.517693281173706\n",
      "2.597177505493164\n",
      "2.511321544647217\n",
      "2.477745294570923\n",
      "2.546208381652832\n",
      "2.5191428661346436\n",
      "2.5863609313964844\n",
      "2.52883243560791\n",
      "2.520193099975586\n",
      "2.6139068603515625\n",
      "2.6195056438446045\n",
      "2.5205183029174805\n",
      "2.611133575439453\n",
      "2.6135525703430176\n",
      "2.62001895904541\n",
      "2.464331865310669\n",
      "2.543865919113159\n",
      "2.480933904647827\n",
      "2.3757340908050537\n",
      "2.6598029136657715\n",
      "2.548009157180786\n",
      "2.365262508392334\n",
      "2.597050905227661\n",
      "2.52538800239563\n",
      "2.503155469894409\n",
      "2.5370676517486572\n",
      "2.571028470993042\n",
      "2.4486165046691895\n",
      "2.5145046710968018\n",
      "2.618922710418701\n",
      "2.472567319869995\n",
      "2.5327770709991455\n",
      "2.467485189437866\n",
      "2.492767095565796\n",
      "2.5595035552978516\n",
      "2.6399571895599365\n",
      "2.473510980606079\n",
      "2.527623414993286\n",
      "2.598677158355713\n",
      "2.5312461853027344\n",
      "2.512885093688965\n",
      "2.4849138259887695\n",
      "2.5553579330444336\n",
      "2.5979270935058594\n",
      "2.4682366847991943\n",
      "2.6035115718841553\n",
      "2.525092363357544\n",
      "2.478724241256714\n",
      "2.578984498977661\n",
      "2.5801804065704346\n",
      "2.556473731994629\n",
      "2.5225231647491455\n",
      "2.5034828186035156\n",
      "2.571807622909546\n",
      "2.5281879901885986\n",
      "2.5857017040252686\n",
      "2.500837802886963\n",
      "2.5550572872161865\n",
      "2.5119471549987793\n",
      "2.520756721496582\n",
      "2.621011257171631\n",
      "2.5830953121185303\n",
      "2.4503402709960938\n",
      "2.513850450515747\n",
      "2.383631944656372\n",
      "2.4503872394561768\n",
      "2.4454562664031982\n",
      "2.4352009296417236\n",
      "2.5314724445343018\n",
      "2.5644171237945557\n",
      "2.5478625297546387\n",
      "2.5246341228485107\n",
      "2.48256254196167\n",
      "2.521273374557495\n",
      "2.619084596633911\n",
      "2.5223517417907715\n",
      "2.515523910522461\n",
      "2.535010576248169\n",
      "2.560826063156128\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for step in range(1000):\n",
    "  xb, yb = get_batch('train')\n",
    "  logits, loss = m(xb, yb)\n",
    "  optimizer.zero_grad(set_to_none=True)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TbCPLmSk5SVZ",
    "outputId": "4944fa0b-43ed-44c1-94c1-f7493c57def1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits before:  torch.Size([1, 1, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 2, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 3, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 4, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 5, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 6, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 7, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 8, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 9, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 10, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 11, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 12, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 13, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 14, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 15, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 16, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 17, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 18, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 19, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 20, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 21, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 22, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 23, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 24, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 25, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 26, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 27, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 28, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 29, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 30, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 31, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 32, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 33, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 34, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 35, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 36, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 37, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 38, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 39, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 40, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 41, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 42, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 43, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 44, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 45, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 46, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 47, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 48, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 49, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 50, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 51, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 52, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 53, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 54, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 55, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 56, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 57, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 58, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 59, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 60, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 61, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 62, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 63, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 64, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 65, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 66, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 67, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 68, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 69, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 70, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 71, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 72, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 73, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 74, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 75, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 76, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 77, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 78, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 79, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 80, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 81, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 82, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 83, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 84, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 85, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 86, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 87, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 88, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 89, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 90, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 91, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 92, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 93, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 94, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 95, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 96, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 97, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 98, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 99, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "Logits before:  torch.Size([1, 100, 49])\n",
      "Logits after:  torch.Size([1, 49])\n",
      "\n",
      "ზონდამა;\n",
      "“რმა ჩნი შაცემა ღლსალი მედაყა სარბონიი მდობავლისაენ მოხლიტთ\n",
      "დოი.\n",
      "მია, ადხლ-ყვავლი მოოს მეღა\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens = 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# The mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "B, T, C = 4, 8, 2\n",
    "# have 4 sequences (B=4).\n",
    "#Each sequence has 8 tokens (T=8).\n",
    "#Each token has 2 possible logits (C=2, vocab size) predicting the next character.\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "  for t in range(T):\n",
    "    xprev = x[b,:t+1]\n",
    "    xbow[b, t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Each row in xbow is average of previous values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5256, -0.7502],\n",
       "        [-0.6540, -1.6095],\n",
       "        [-0.1002, -0.6092],\n",
       "        [-0.9798, -1.6091],\n",
       "        [-0.7121,  0.3037],\n",
       "        [-0.7773, -0.2515],\n",
       "        [-0.2223,  1.6871],\n",
       "        [ 0.2284,  0.4676]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5256, -0.7502],\n",
       "        [-1.0898, -1.1799],\n",
       "        [-0.7599, -0.9896],\n",
       "        [-0.8149, -1.1445],\n",
       "        [-0.7943, -0.8549],\n",
       "        [-0.7915, -0.7543],\n",
       "        [-0.7102, -0.4055],\n",
       "        [-0.5929, -0.2964]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      "b=\n",
      "tensor([[5., 9.],\n",
      "        [4., 8.],\n",
      "        [3., 3.]])\n",
      "--\n",
      "c=\n",
      "tensor([[12., 20.],\n",
      "        [12., 20.],\n",
      "        [12., 20.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "a = torch.ones(3, 3)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "\n",
    "c = a @ b\n",
    "\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      "b=\n",
      "tensor([[5., 9.],\n",
      "        [4., 8.],\n",
      "        [3., 3.]])\n",
      "--\n",
      "c=\n",
      "tensor([[ 5.,  9.],\n",
      "        [ 9., 17.],\n",
      "        [12., 20.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "\n",
    "c = a @ b\n",
    "\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a previously = \n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "torch.sum(a, 1, keepdim=True) = \n",
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[5., 9.],\n",
      "        [4., 8.],\n",
      "        [3., 3.]])\n",
      "--\n",
      "c=\n",
      "tensor([[5.0000, 9.0000],\n",
      "        [4.5000, 8.5000],\n",
      "        [4.0000, 6.6667]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "print('a previously = ')\n",
    "print(a)\n",
    "print('torch.sum(a, 1, keepdim=True) = ')\n",
    "print(torch.sum(a, 1, keepdim=True))\n",
    "a = a/torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "\n",
    "c = a @ b\n",
    "\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# version 1\n",
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "  for t in range(T):\n",
    "    xprev = x[b,:t+1]\n",
    "    xbow[b, t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wei=\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]]) torch.Size([8, 8])\n",
      "x=\n",
      "tensor([[[-1.5256, -0.7502],\n",
      "         [-0.6540, -1.6095],\n",
      "         [-0.1002, -0.6092],\n",
      "         [-0.9798, -1.6091],\n",
      "         [-0.7121,  0.3037],\n",
      "         [-0.7773, -0.2515],\n",
      "         [-0.2223,  1.6871],\n",
      "         [ 0.2284,  0.4676]],\n",
      "\n",
      "        [[-0.6970, -1.1608],\n",
      "         [ 0.6995,  0.1991],\n",
      "         [ 0.8657,  0.2444],\n",
      "         [-0.6629,  0.8073],\n",
      "         [ 1.1017, -0.1759],\n",
      "         [-2.2456, -1.4465],\n",
      "         [ 0.0612, -0.6177],\n",
      "         [-0.7981, -0.1316]],\n",
      "\n",
      "        [[ 1.8793, -0.0721],\n",
      "         [ 0.1578, -0.7735],\n",
      "         [ 0.1991,  0.0457],\n",
      "         [ 0.1530, -0.4757],\n",
      "         [-0.1110,  0.2927],\n",
      "         [-0.1578, -0.0288],\n",
      "         [ 2.3571, -1.0373],\n",
      "         [ 1.5748, -0.6298]],\n",
      "\n",
      "        [[-0.9274,  0.5451],\n",
      "         [ 0.0663, -0.4370],\n",
      "         [ 0.7626,  0.4415],\n",
      "         [ 1.1651,  2.0154],\n",
      "         [ 0.1374,  0.9386],\n",
      "         [-0.1860, -0.6446],\n",
      "         [ 1.5392, -0.8696],\n",
      "         [-3.3312, -0.7479]]]) torch.Size([4, 8, 2])\n",
      "tensor([[[-1.5256e+00, -7.5023e-01],\n",
      "         [-1.0898e+00, -1.1799e+00],\n",
      "         [-7.5991e-01, -9.8964e-01],\n",
      "         [-8.1488e-01, -1.1445e+00],\n",
      "         [-7.9433e-01, -8.5486e-01],\n",
      "         [-7.9150e-01, -7.5429e-01],\n",
      "         [-7.1018e-01, -4.0552e-01],\n",
      "         [-5.9285e-01, -2.9637e-01]],\n",
      "\n",
      "        [[-6.9697e-01, -1.1608e+00],\n",
      "         [ 1.2850e-03, -4.8084e-01],\n",
      "         [ 2.8942e-01, -2.3909e-01],\n",
      "         [ 5.1338e-02,  2.2508e-02],\n",
      "         [ 2.6141e-01, -1.7181e-02],\n",
      "         [-1.5642e-01, -2.5539e-01],\n",
      "         [-1.2534e-01, -3.0716e-01],\n",
      "         [-2.0943e-01, -2.8522e-01]],\n",
      "\n",
      "        [[ 1.8793e+00, -7.2132e-02],\n",
      "         [ 1.0186e+00, -4.2279e-01],\n",
      "         [ 7.4539e-01, -2.6663e-01],\n",
      "         [ 5.9728e-01, -3.1889e-01],\n",
      "         [ 4.5562e-01, -1.9657e-01],\n",
      "         [ 3.5338e-01, -1.6860e-01],\n",
      "         [ 6.3963e-01, -2.9271e-01],\n",
      "         [ 7.5652e-01, -3.3485e-01]],\n",
      "\n",
      "        [[-9.2739e-01,  5.4514e-01],\n",
      "         [-4.3056e-01,  5.4051e-02],\n",
      "         [-3.2837e-02,  1.8320e-01],\n",
      "         [ 2.6666e-01,  6.4125e-01],\n",
      "         [ 2.4081e-01,  7.0073e-01],\n",
      "         [ 1.6967e-01,  4.7650e-01],\n",
      "         [ 3.6533e-01,  2.8420e-01],\n",
      "         [-9.6734e-02,  1.5519e-01]]]) torch.Size([4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "# version 2\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "print('wei=')\n",
    "print(wei, wei.shape)\n",
    "print('x=')\n",
    "print(x, x.shape)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) -----> (B, T, C)\n",
    "print(xbow2, xbow2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5256, -0.7502],\n",
       "        [-1.0898, -1.1799],\n",
       "        [-0.7599, -0.9896],\n",
       "        [-0.8149, -1.1445],\n",
       "        [-0.7943, -0.8549],\n",
       "        [-0.7915, -0.7543],\n",
       "        [-0.7102, -0.4055],\n",
       "        [-0.5929, -0.2964]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5256, -0.7502],\n",
       "        [-1.0898, -1.1799],\n",
       "        [-0.7599, -0.9896],\n",
       "        [-0.8149, -1.1445],\n",
       "        [-0.7943, -0.8549],\n",
       "        [-0.7915, -0.7543],\n",
       "        [-0.7102, -0.4055],\n",
       "        [-0.5929, -0.2964]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = F.softmax(wei, dim = -1)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# version 3\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim = -1)\n",
    "xbow3 = wei @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.5256e+00, -7.5023e-01],\n",
       "         [-1.0898e+00, -1.1799e+00],\n",
       "         [-7.5991e-01, -9.8964e-01],\n",
       "         [-8.1488e-01, -1.1445e+00],\n",
       "         [-7.9433e-01, -8.5486e-01],\n",
       "         [-7.9150e-01, -7.5429e-01],\n",
       "         [-7.1018e-01, -4.0552e-01],\n",
       "         [-5.9285e-01, -2.9637e-01]],\n",
       "\n",
       "        [[-6.9697e-01, -1.1608e+00],\n",
       "         [ 1.2850e-03, -4.8084e-01],\n",
       "         [ 2.8942e-01, -2.3909e-01],\n",
       "         [ 5.1338e-02,  2.2508e-02],\n",
       "         [ 2.6141e-01, -1.7181e-02],\n",
       "         [-1.5642e-01, -2.5539e-01],\n",
       "         [-1.2534e-01, -3.0716e-01],\n",
       "         [-2.0943e-01, -2.8522e-01]],\n",
       "\n",
       "        [[ 1.8793e+00, -7.2132e-02],\n",
       "         [ 1.0186e+00, -4.2279e-01],\n",
       "         [ 7.4539e-01, -2.6663e-01],\n",
       "         [ 5.9728e-01, -3.1889e-01],\n",
       "         [ 4.5562e-01, -1.9657e-01],\n",
       "         [ 3.5338e-01, -1.6860e-01],\n",
       "         [ 6.3963e-01, -2.9271e-01],\n",
       "         [ 7.5652e-01, -3.3485e-01]],\n",
       "\n",
       "        [[-9.2739e-01,  5.4514e-01],\n",
       "         [-4.3056e-01,  5.4051e-02],\n",
       "         [-3.2837e-02,  1.8320e-01],\n",
       "         [ 2.6666e-01,  6.4125e-01],\n",
       "         [ 2.4081e-01,  7.0073e-01],\n",
       "         [ 1.6967e-01,  4.7650e-01],\n",
       "         [ 3.6533e-01,  2.8420e-01],\n",
       "         [-9.6734e-02,  1.5519e-01]]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1)\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias = False)\n",
    "query = nn.Linear(C, head_size, bias = False)\n",
    "value = nn.Linear(C, head_size, bias = False)\n",
    "\n",
    "k = key(x) # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) #(B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril ==0, float('-inf')) # If we do decoder block we have this line but if we would encoder block then we do not need this line.\n",
    "wei = F.softmax(wei, -1)\n",
    "\n",
    "v = value(x)\n",
    "\n",
    "out = wei @ v\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [9.5482e-01, 4.5181e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [1.5075e-01, 2.4786e-01, 6.0139e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [6.0910e-01, 6.0784e-02, 1.3287e-01, 1.9725e-01, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [1.1235e-01, 9.3733e-04, 1.5950e-01, 2.2520e-01, 5.0200e-01, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [8.1421e-02, 2.6164e-02, 3.0799e-01, 2.3788e-01, 3.0135e-01, 4.5193e-02,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [5.5458e-02, 3.9424e-01, 3.3161e-02, 3.7227e-02, 2.6962e-02, 2.5549e-01,\n",
       "         1.9746e-01, 0.0000e+00],\n",
       "        [4.4814e-01, 1.0565e-02, 1.0351e-01, 4.2752e-02, 6.5090e-02, 4.0655e-02,\n",
       "         2.8155e-01, 7.7372e-03]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1)# * head_size ** -0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0965)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1010)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.6169)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
